{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [ロボットアーム](#toc1_)    \n",
    "- [pybulletの起動](#toc2_)    \n",
    "- [pybulletの初期設定](#toc3_)    \n",
    "- [ロボットアームの生成](#toc4_)    \n",
    "- [カラーのオブジェクトの生成](#toc5_)    \n",
    "- [関数の定義](#toc6_)    \n",
    "- [カメラの設定](#toc7_)    \n",
    "- [アームの初期姿勢の設定](#toc8_)    \n",
    "- [カメラ座標系におけるカラーのオブジェクトの位置を推定](#toc9_)    \n",
    "- [ワールド座標系におけるカラーのオブジェクトの位置を推定](#toc10_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[ロボットアーム](#toc0_)\n",
    "\n",
    "本notebookでは6軸のロボットアームを生成し、「アームの手先のカメラ」から、指定した色の物体の位置を推定する方法を解説します。\n",
    "\n",
    "（pybulletで使用可能な関数がまとめられたマニュアルについては[こちら](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf)を参照してください。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[pybulletの起動](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import pybullet\n",
    "import pybullet_data\n",
    "physicsClient = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[pybulletの初期設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # シミュレーション空間をリセット\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # pybulletに必要なデータへのパスを追加\n",
    "pybullet.setGravity(0.0, 0.0, -9.8) # 地球上における重力に設定\n",
    "timeStep = 1./240.\n",
    "pybullet.setTimeStep(timeStep) # 1stepあたりに経過する時間の設定\n",
    "\n",
    "#床の読み込み\n",
    "planeId = pybullet.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# GUIモードの際のカメラの位置などを設定\n",
    "cameraDistance = 3.5\n",
    "cameraYaw = 180.0 # deg\n",
    "cameraPitch = -40 # deg\n",
    "cameraTargetPosition = [0, 0.5, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(cameraDistance, cameraYaw, cameraPitch, cameraTargetPosition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[ロボットアームの生成](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: target_position_vertual_link\n"
     ]
    }
   ],
   "source": [
    "# ロボットの読み込み\n",
    "armStartPos = [0, 0, 0.0]  # 初期位置(x,y,z)を設定\n",
    "armStartOrientation = pybullet.getQuaternionFromEuler([0,0,0])  # 初期姿勢(roll, pitch, yaw)を設定\n",
    "armId = pybullet.loadURDF(\"../urdf/simple6d_arm_with_gripper.urdf\", armStartPos, armStartOrientation, useFixedBase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[カラーのオブジェクトの生成](#toc0_)\n",
    "\n",
    "カメラで検出したいカラーのオブジェクトを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここを変えると結果が変わります（手先のカメラの画角に収まるように設定してください）####\n",
    "colorBoxPos = [-0.5, 0.2, 0.05] # 色物体の初期位置(x, y, z)を設定\n",
    "#################################################################################\n",
    "\n",
    "colorBoxId = pybullet.loadURDF(\"../urdf/simple_box.urdf\", colorBoxPos, pybullet.getQuaternionFromEuler([0.0, 0.0, 0.0]), globalScaling=0.1, useFixedBase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[関数の定義](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def DetectColorObjPose(targetRGB, rgbImg, depthImg):\n",
    "    \"\"\"\n",
    "    最初に検出された色物体の中心位置、深度、姿勢を取得する関数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    targetRGB : list\n",
    "        検出したい色のRGB\n",
    "    rgbImg : numpy.ndarray\n",
    "        カメラ画像（RGB）\n",
    "    depthImg : numpy.ndarray\n",
    "        カメラ画像（Depth）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj_pose : numpy.ndarray\n",
    "        色物体の位置と姿勢（x, y, z, roll, pitch, yaw）\n",
    "    \"\"\"\n",
    "\n",
    "    # カメラ画像をHSV形式に変換\n",
    "    hsvImg = cv2.cvtColor(rgbImg, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # RGBをHSVに変換\n",
    "    targetHsv = cv2.cvtColor(np.uint8([[targetRGB]]), cv2.COLOR_RGB2HSV)[0][0]\n",
    "\n",
    "    # 検出したい色の範囲を指定\n",
    "    lower = np.array([targetHsv[0]-10, 50, 50])\n",
    "    upper = np.array([targetHsv[0]+10, 255, 255])\n",
    "\n",
    "    # 指定した色のみを抽出\n",
    "    mask = cv2.inRange(hsvImg, lower, upper)\n",
    "\n",
    "    # 輪郭を抽出\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 最大面積の輪郭を取得\n",
    "    max_area = 0\n",
    "    max_area_contour = None\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_area_contour = contour\n",
    "\n",
    "    # 輪郭が見つからなかった場合\n",
    "    if max_area_contour is None:\n",
    "        return None\n",
    "    \n",
    "    # 輪郭の中心位置を取得\n",
    "    M = cv2.moments(max_area_contour)\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "\n",
    "    # 輪郭の中心位置における深度を取得\n",
    "    depth = depthImg[cy, cx]\n",
    "\n",
    "    pos = [cx, cy, depth]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "\n",
    "\n",
    "def Rx(theta):\n",
    "    \"\"\"\n",
    "    x軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        x軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    \"\"\"\n",
    "    y軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        y軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    \"\"\"\n",
    "    z軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        z軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "def Hx(theta):\n",
    "    \"\"\"\n",
    "    x軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        x軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta), 0],\n",
    "                     [0, np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hy(theta):\n",
    "    \"\"\"\n",
    "    y軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        y軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta), 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hz(theta):\n",
    "    \"\"\"\n",
    "    z軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        z軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0, 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hp(x, y, z):\n",
    "    \"\"\"\n",
    "    平行移動の同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        x方向の移動量\n",
    "    y : float\n",
    "        y方向の移動量\n",
    "    z : float\n",
    "        z方向の移動量\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        平行移動の同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[カメラの設定](#toc0_)\n",
    "カメラに関する設定（焦点距離、内部パラメータなど）を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラ設定\n",
    "fov = 60 # Pybulletでは、垂直方向のfovを指定\n",
    "imageWidth = 224 # 画像の幅\n",
    "imageHeight = 224 # 画像の高さ\n",
    "aspect = imageWidth / imageHeight # アスペクト比\n",
    "near = 0.05 # カメラの最小距離\n",
    "far = 5 # カメラの最大距離\n",
    "projectionMatrix = pybullet.computeProjectionMatrixFOV(fov, aspect, near, far) # 射影行列を計算\n",
    "\n",
    "# 焦点距離を求める\n",
    "fovRad = np.deg2rad(fov)\n",
    "f = (imageHeight / 2) / np.tan(fovRad / 2)\n",
    "\n",
    "# カメラの内部パラメータ\n",
    "cameraMatrix = np.array([[f, 0, imageWidth//2],\n",
    "                         [0, f, imageHeight//2],\n",
    "                         [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# 歪み係数（ここでは、歪みがないと仮定）\n",
    "distCoeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[アームの初期姿勢の設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロボットの各関節のインデックス\n",
    "link1JointIdx = 0\n",
    "link2JointIdx = 1\n",
    "link3JointIdx = 2\n",
    "link4JointIdx = 3\n",
    "link5JointIdx = 4\n",
    "link6JointIdx = 5\n",
    "cameraIdx = 6\n",
    "cameraTargetIdx = 7\n",
    "\n",
    "# 各リンクの長さ\n",
    "worldToArmOriginX = 0.0 # ワールド座標系から見たアームの原点のx座標\n",
    "worldToArmOriginY = 0.0 # ワールド座標系から見たアームの原点のy座標\n",
    "worldToArmOriginZ = 0.8 # ワールド座標系から見たアームの原点のz座標(「simple6d_arm_with_gripper.urdf」の base_link の z方向の長さ)\n",
    "link1Length = 0.3 # link1の長さ(「simple6d_arm_with_gripper.urdf」の link1 の z方向の長さ)\n",
    "link2Length = 0.5 # link2の長さ(「simple6d_arm_with_gripper.urdf」の link2 の z方向の長さ)\n",
    "link3Length = 0.5 # link3の長さ(「simple6d_arm_with_gripper.urdf」の link3 の z方向の長さ)\n",
    "link4Length = 0.1 # link4の長さ(「simple6d_arm_with_gripper.urdf」の link4 の z方向の長さ)\n",
    "link5Length = 0.1 # link5の長さ(「simple6d_arm_with_gripper.urdf」の link5 の z方向の長さ)\n",
    "link6Length = 0.15 # link6の長さ(「simple6d_arm_with_gripper.urdf」の link6 の z方向の長さ)\n",
    "cameraZSize = 0.01 # カメラのz方向の長さ\n",
    "cameraXOffset = 0.08 # カメラのリンク6座標系の原点からのx方向のオフセット\n",
    "\n",
    "# ここを変えると結果が変わります（手先のカメラの画角にcolorBoxが収まるように設定してください）####\n",
    "# 各関節の角度\n",
    "joint1Angle = -15.0\n",
    "joint2Angle = -80.0\n",
    "joint3Angle = -100.0\n",
    "joint4Angle = 0.0\n",
    "joint5Angle = 0.0\n",
    "joint6Angle = 10.0\n",
    "########################################################################################\n",
    "\n",
    "# 各関節の角度をラジアンに変換\n",
    "q1 = np.deg2rad(joint1Angle)\n",
    "q2 = np.deg2rad(joint2Angle)\n",
    "q3 = np.deg2rad(joint3Angle)\n",
    "q4 = np.deg2rad(joint4Angle)\n",
    "q5 = np.deg2rad(joint5Angle)\n",
    "q6 = np.deg2rad(joint6Angle)\n",
    "\n",
    "# 各関節の角度を設定\n",
    "pybullet.resetJointState(armId, link1JointIdx, q1)\n",
    "pybullet.resetJointState(armId, link2JointIdx, q2)\n",
    "pybullet.resetJointState(armId, link3JointIdx, q3)\n",
    "pybullet.resetJointState(armId, link4JointIdx, q4)\n",
    "pybullet.resetJointState(armId, link5JointIdx, q5)\n",
    "pybullet.resetJointState(armId, link6JointIdx, q6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[カメラ座標系におけるカラーのオブジェクトの位置を推定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラの位置を取得\n",
    "cameraLinkPose = pybullet.getLinkState(armId, cameraIdx)[0] # 手先のカメラリンクの位置\n",
    "cameraTargetLinkPose = pybullet.getLinkState(armId, cameraTargetIdx)[0] # カメラリンクの少しだけ前に設定した仮想的なリンクの位置\n",
    "cameraLinkOrientation = pybullet.getEulerFromQuaternion(pybullet.getLinkState(armId, cameraIdx)[1]) # 手先のカメラリンクの姿勢\n",
    "\n",
    "# ロボットアームの姿勢に合わせて、カメラの上方向のベクトルを設定\n",
    "RW1 = Rz(theta=0)@Ry(theta=0)@Rx(theta=0) # ワールド座標系 -> link1座標系の回転行列\n",
    "R12 = Rz(theta=q1) # link1座標系 -> link2座標系\n",
    "R23 = Ry(theta=q2) # link2座標系 -> link3座標系\n",
    "R34 = Ry(theta=q3) # link3座標系 -> link4座標系\n",
    "R45 = Rz(theta=q4) # link4座標系 -> link5座標系\n",
    "R56 = Ry(theta=q5) # link5座標系 -> link6座標系\n",
    "R6C = Rz(theta=q6 + np.deg2rad(-90.0)) # link6座標系 -> カメラ座標系（カメラを-90度回転すると、取得される画像が正しい向きになる）\n",
    "R = RW1@ R12@R23@R34@R45@R56@R6C # link1座標系 -> カメラ座標系への回転行列\n",
    "cameraUpVector = np.array([0, -1, 0]) # デフォルトのカメラの上方向のベクトル\n",
    "rotateCameraUpVector = R@cameraUpVector # ロボットアームの姿勢に合わせたカメラの上方向のベクトル\n",
    "\n",
    "# カメラのビュー行列を計算\n",
    "viewMatrix = pybullet.computeViewMatrix(cameraEyePosition=[cameraLinkPose[0], cameraLinkPose[1], cameraLinkPose[2]],cameraTargetPosition=[cameraTargetLinkPose[0], cameraTargetLinkPose[1], cameraTargetLinkPose[2]],cameraUpVector=[rotateCameraUpVector[0], rotateCameraUpVector[1], rotateCameraUpVector[2]])\n",
    "\n",
    "# カメラ画像を取得\n",
    "_, _, rgbImg, depthImg, _ = pybullet.getCameraImage(\n",
    "    width=imageWidth,\n",
    "    height=imageHeight,\n",
    "    viewMatrix=viewMatrix,\n",
    "    projectionMatrix=projectionMatrix,\n",
    "    renderer=pybullet.ER_BULLET_HARDWARE_OPENGL\n",
    ")\n",
    "\n",
    "# 指定した色の物体の位置を取得\n",
    "detectColorRGB = [0, 0, 255] # 検出したい色を指定（ここでは青色）\n",
    "pos = DetectColorObjPose(detectColorRGB, rgbImg, depthImg)\n",
    "pixelX = pos[0] # x座標（画像上の位置）\n",
    "pixelY = pos[1] # y座標（画像上の位置）\n",
    "normalZ = pos[2] # z座標（0～1に正規化された深度）\n",
    "z = far * near / (far - (far - near) * normalZ) # 0～1に正規化された深度を距離[m]に変換\n",
    "x = (pixelX - imageWidth//2) * z / f # pixelXを距離[m]に変換\n",
    "y = (pixelY - imageHeight//2) * z / f # pixelYを距離[m]に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[ワールド座標系におけるカラーのオブジェクトの位置を推定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color box obj pose:  -0.5 0.2 0.05\n",
      "estimate color obj pose -0.4979110352358044 0.201106116292795 0.10000000370971596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ワールド座標系から見たカメラの位置を求める\n",
    "TW1 = Hz(theta=0)@Hy(theta=0)@Hx(theta=0)@Hp(x=worldToArmOriginX, y=worldToArmOriginY, z=worldToArmOriginZ) # ワールド座標系 -> link1座標系の同次変換行列\n",
    "T12 = Hz(theta=q1)@Hp(x=0, y=0, z=link1Length) # link1座標系 -> link2座標系\n",
    "T23 = Hy(theta=q2)@Hp(x=0, y=0, z=link2Length) # link2座標系 -> link3座標系\n",
    "T34 = Hy(theta=q3)@Hp(x=0, y=0, z=link3Length) # link3座標系 -> link4座標系\n",
    "T45 = Hz(theta=q4)@Hp(x=0, y=0, z=link4Length) # link4座標系 -> link5座標系\n",
    "T56 = Hy(theta=q5)@Hp(x=0, y=0, z=link5Length) # link5座標系 -> link6座標系\n",
    "T6C = Hz(theta=q6)@Hp(x=cameraXOffset, y=0, z=link6Length-cameraZSize/2) # link6座標系 -> カメラ座標系\n",
    "originPos = np.array([0, 0, 0, 1]) # 原点位置\n",
    "cameraPosWorld = TW1@T12@T23@T34@T45@T56@T6C@originPos # ワールド座標系から見たカメラ座標系の原点位置\n",
    "cameraPosWorld = cameraPosWorld[:3] # 同次変換行列によって求めた座標は[x, y, z, 1]の形式なので、[x, y, z]の形式に変換\n",
    "colorObjPosCamera = np.array([x, y, z]) # カメラ座標系から見た「カラーのオブジェクト」の中心座標\n",
    "colorObjPosWorld = cameraPosWorld + R@colorObjPosCamera # ワールド座標系から見た「カラーオブジェクト」の位置\n",
    "\n",
    "# Pybuletの画面上に、ワールド座標系から見た「手先カメラ位置」→「推定したカラーのオブジェクトの位置」に向かう赤色の線を描画\n",
    "pybullet.addUserDebugLine(cameraPosWorld, colorObjPosWorld, [1, 0, 0], lineWidth=2)\n",
    "\n",
    "# ボックスの「真の位置」と「推定した位置」を表示\n",
    "boxPos, _ = pybullet.getBasePositionAndOrientation(colorBoxId)\n",
    "pybullet.addUserDebugText(f\"true box pose ({boxPos[0]:.3f}, {boxPos[1]:.3f}, {boxPos[2]:.3f})\", [2.0, 0.5, 0], textColorRGB=[1, 0, 0], textSize=1.3)\n",
    "pybullet.addUserDebugText(f\"eye in hand, estimate box pose ({colorObjPosWorld[0]:.3f}, {colorObjPosWorld[1]:.3f}, {colorObjPosWorld[2]:.3f})\", [2.0, 1.0, 0.0], textColorRGB=[1, 0, 0], textSize=1.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
