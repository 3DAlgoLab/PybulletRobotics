{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [image based visual servo (IBVS)](#toc1_)    \n",
    "- [Starting pybullet](#toc2_)    \n",
    "- [Initial Setup for pybullet](#toc3_)    \n",
    "- [Generating the Robot Arm](#toc4_)    \n",
    "- [Generating the Box with AR Marker Attached](#toc5_)    \n",
    "- [Changing the Light Source Position](#toc6_)    \n",
    "- [Defining Functions to be Used](#toc7_)    \n",
    "- [Setting Camera Parameters](#toc8_)    \n",
    "- [Setting Parameters for AR Markers](#toc9_)    \n",
    "- [Setting Parameters for the Robot Arm](#toc10_)    \n",
    "- [Executing Visual Servoing](#toc11_)    \n",
    "- [Bonus: Drawing Feature Point Positions](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[image based visual servo (IBVS)](#toc0_)\n",
    "\n",
    "In this notebook, we will introduce a method to perform image-based visual servoing using a 6-axis robot arm.\n",
    "\n",
    "Visual servoing reference: https://github.com/RiddhimanRaut/Ur5_Visual_Servoing/blob/master/ur5_control_nodes/src/vs_ur5.py\n",
    "\n",
    "(For a manual summarizing the functions available in pybullet, refer to [this link](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Starting pybullet](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import pybullet\n",
    "import pybullet_data\n",
    "physics_client = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Initial Setup for pybullet](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # Reset the simulation space\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # Add paths to necessary data for pybullet\n",
    "# pybullet.setGravity(0.0, 0.0, -9.8) # Set gravity as on Earth (for simplicity, gravity is not considered this time)\n",
    "time_step = 1./240.\n",
    "pybullet.setTimeStep(time_step) # Set the time elapsed per step\n",
    "\n",
    "# Load the floor\n",
    "plane_id = pybullet.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Set the camera position and other parameters in GUI mode\n",
    "camera_distance = 2.0\n",
    "camera_yaw = 0.0 # deg\n",
    "camera_pitch = -20 # deg\n",
    "camera_target_position = [0.0, 0.0, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Generating the Robot Arm](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: target_position_vertual_link\n"
     ]
    }
   ],
   "source": [
    "# Load the robot\n",
    "arm_start_pos = [0, 0, 0]  # Set the initial position (x, y, z)\n",
    "arm_start_orientation = pybullet.getQuaternionFromEuler([0, 0, 0])  # Set the initial orientation (roll, pitch, yaw)\n",
    "arm_id = pybullet.loadURDF(\"../urdf/simple6d_arm_with_force_sensor.urdf\", arm_start_pos, arm_start_orientation, useFixedBase=True) # Fix the root link with useFixedBase=True to prevent the robot from falling\n",
    "\n",
    "# Set the camera position and other parameters in GUI mode\n",
    "camera_distance = 1.5\n",
    "camera_yaw = 50.0 # deg\n",
    "camera_pitch = -40 # deg\n",
    "camera_target_position = [0.0, 0.0, 1.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Generating the Box with AR Marker Attached](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a box with an AR marker drawn on one side\n",
    "box_bid = pybullet.loadURDF(\"../urdf/ar_marker_box.urdf\", [0, 0, 0.5], useFixedBase=True)\n",
    "\n",
    "# Set the texture (specify the same one as in the urdf file)\n",
    "texture_id = pybullet.loadTexture(\"../texture/ar_marker_box.png\")\n",
    "pybullet.changeVisualShape(box_bid, -1, textureUniqueId=texture_id)\n",
    "\n",
    "# Set sliders to adjust the position of the box with the AR marker attached\n",
    "pybullet.addUserDebugParameter(\"obj_x\", -4, 4, 0.0)\n",
    "pybullet.addUserDebugParameter(\"obj_y\", -2, 0, -1.5)\n",
    "pybullet.addUserDebugParameter(\"obj_z\", -4, 8, 1.3)\n",
    "pybullet.addUserDebugParameter(\"obj_roll\", -3.14, 3.14, 0.0)\n",
    "pybullet.addUserDebugParameter(\"obj_pitch\", -3.14, 3.14, 0)\n",
    "pybullet.addUserDebugParameter(\"obj_yaw\", -3.14, 3.14, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Changing the Light Source Position](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the light source position because the AR marker is hard to recognize with the default light source position\n",
    "pybullet.configureDebugVisualizer(lightPosition=[0, 0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Defining Functions to be Used](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def Rx(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the x-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the x-axis\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the y-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the y-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the z-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the z-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "\n",
    "def Hx(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the x-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the x-axis\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta), 0],\n",
    "                     [0, np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hy(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the y-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the y-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta), 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hz(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the z-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the z-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0, 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hp(x, y, z):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix for translation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Translation in the x direction\n",
    "    y : float\n",
    "        Translation in the y direction\n",
    "    z : float\n",
    "        Translation in the z direction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix for translation\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]])\n",
    "                     \n",
    "def make_6d_jacobian_matrix(R, o1, o2, o3, o4, o5, o6, oc):\n",
    "    \"\"\"\n",
    "    Calculate the basic Jacobian matrix of a 6-axis robot arm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    R : numpy.ndarray\n",
    "        Rotation matrices between coordinate systems of the robot arm\n",
    "    o1 : numpy.ndarray\n",
    "        Origin of link1\n",
    "    o2 : numpy.ndarray\n",
    "        Origin of link2\n",
    "    o3 : numpy.ndarray\n",
    "        Origin of link3\n",
    "    o4 : numpy.ndarray\n",
    "        Origin of link4\n",
    "    o5 : numpy.ndarray\n",
    "        Origin of link5\n",
    "    o6 : numpy.ndarray\n",
    "        Origin of link6\n",
    "    oc : numpy.ndarray\n",
    "        Origin of the camera\n",
    "   \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Jv : numpy.ndarray\n",
    "         Basic Jacobian matrix\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    R12 = R[0]\n",
    "    R23 = R[1]\n",
    "    R34 = R[2]\n",
    "    R45 = R[3]\n",
    "    R56 = R[4]\n",
    "    R6C = R[5]\n",
    "    ex = np.array([1, 0, 0])\n",
    "    ey = np.array([0, 1, 0])\n",
    "    ez = np.array([0, 0, 1])\n",
    "\n",
    "    # Calculate each element of the Jacobian matrix\n",
    "    a1 = R12 @ ez\n",
    "    a2 = R12 @ R23 @ ey\n",
    "    a3 = R12 @ R23 @ R34 @ ey\n",
    "    a4 = R12 @ R23 @ R34 @ R45 @ ez\n",
    "    a5 = R12 @ R23 @ R34 @ R45 @ R56 @ ey\n",
    "    a6 = R12 @ R23 @ R34 @ R45 @ R56 @ R6C @ ez\n",
    "\n",
    "    # Ji = [[ai x (oc - oi)], \n",
    "    #       [ai]]\n",
    "    j1 = np.concatenate((np.cross(a1, oc-o1).reshape(3, 1), a1.reshape(3, 1)), axis=0)\n",
    "    j2 = np.concatenate((np.cross(a2, oc-o2).reshape(3, 1), a2.reshape(3, 1)), axis=0)\n",
    "    j3 = np.concatenate((np.cross(a3, oc-o3).reshape(3, 1), a3.reshape(3, 1)), axis=0)\n",
    "    j4 = np.concatenate((np.cross(a4, oc-o4).reshape(3, 1), a4.reshape(3, 1)), axis=0)\n",
    "    j5 = np.concatenate((np.cross(a5, oc-o5).reshape(3, 1), a5.reshape(3, 1)), axis=0)\n",
    "    j6 = np.concatenate((np.cross(a6, oc-o6).reshape(3, 1), a6.reshape(3, 1)), axis=0)\n",
    "\n",
    "    Jv = np.concatenate((j1, j2, j3, j4, j5, j6), axis=1)\n",
    "    return Jv\n",
    "\n",
    "def make_4features_image_jacobian_matrix(current_feature_points, camera_to_obj, f):\n",
    "    \"\"\"\n",
    "    Calculate the image Jacobian matrix for 4 feature points\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_feature_points : numpy.ndarray\n",
    "        Current feature point coordinates\n",
    "    camera_to_objs : float\n",
    "        Distance from the camera to the object\n",
    "    f : float\n",
    "        Focal length of the camera\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Ji : numpy.ndarray\n",
    "        Image Jacobian matrix\n",
    "    \"\"\"\n",
    "    # Image Jacobian matrix for 4 feature points\n",
    "    u1 = current_feature_points[0]\n",
    "    v1 = current_feature_points[1]\n",
    "    u2 = current_feature_points[2]\n",
    "    v2 = current_feature_points[3]\n",
    "    u3 = current_feature_points[4]\n",
    "    v3 = current_feature_points[5]\n",
    "    u4 = current_feature_points[6]\n",
    "    v4 = current_feature_points[7]\n",
    "\n",
    "    Ji = np.array([[-f*(1/camera_to_obj), 0, u1*(1/camera_to_obj), u1*v1/f, -(f+(1/f)*u1**2), v1],\n",
    "                   [0, -f*(1/camera_to_obj), v1*(1/camera_to_obj), f+(1/f)*v1**2, -(1/f)*u1*v1, -u1],\n",
    "                   [-f*(1/camera_to_obj), 0, u2*(1/camera_to_obj), u2*v2/f, -(f+(1/f)*u2**2), v2],\n",
    "                   [0, -f*(1/camera_to_obj), v2*(1/camera_to_obj), f+(1/f)*v2**2, -(1/f)*u2*v2, -u2],\n",
    "                   [-f*(1/camera_to_obj), 0, u3*(1/camera_to_obj), u3*v3/f, -(f+(1/f)*u3**2), v3],\n",
    "                   [0, -f*(1/camera_to_obj), v3*(1/camera_to_obj), f+(1/f)*v3**2, -(1/f)*u3*v3, -u3],\n",
    "                   [-f*(1/camera_to_obj), 0, u4*(1/camera_to_obj), u4*v4/f, -(f+(1/f)*u4**2), v4],\n",
    "                   [0, -f*(1/camera_to_obj), v4*(1/camera_to_obj), f+(1/f)*v4**2, -(1/f)*u4*v4, -u4]])\n",
    "    return Ji\n",
    "\n",
    "def detect_ar_marker_corner_pos_and_depth(marker_size, aruco_dict, parameters, rgb_img, camera_matrix, dist_coeffs):\n",
    "    \"\"\"\n",
    "    Detect the positions of the four corners of the AR marker and the distance to the camera\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    marker_size : float\n",
    "        Length of one side of the AR marker (meters)\n",
    "    aruco_dict : cv2.aruco.Dictionary\n",
    "        Dictionary of AR markers\n",
    "    parameters : cv2.aruco.DetectorParameters\n",
    "        Parameters for AR marker detection\n",
    "    rgb_img : numpy.ndarray\n",
    "        Camera image (RGB)\n",
    "    camera_matrix : numpy.ndarray\n",
    "        Camera intrinsic parameter matrix\n",
    "    dist_coeffs : numpy.ndarray\n",
    "        Distortion coefficients\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corner_pos : numpy.ndarray\n",
    "        Positions of the four corners of the AR marker (x1, y1, x2, y2, x3, y3, x4, y4) [pixel positions]\n",
    "    depth : float\n",
    "        Distance from the camera to the AR marker [m]\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the camera image to grayscale\n",
    "    rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect AR markers\n",
    "    corners, ids, _ = cv2.aruco.detectMarkers(rgb_img, aruco_dict, parameters=parameters)\n",
    "\n",
    "    # Return None if no markers are detected\n",
    "    if ids is None or len(ids) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    # Get the positions of the four corners of the first detected marker\n",
    "    marker_corners = corners[0][0]\n",
    "\n",
    "    # Get the positions and corresponding depths of each corner\n",
    "    corner_pos = np.zeros(8)\n",
    "    for i in range(4):\n",
    "        x, y = int(marker_corners[i][0]), int(marker_corners[i][1])\n",
    "        corner_pos[i*2] = x\n",
    "        corner_pos[i*2+1] = y\n",
    "\n",
    "    # Calculate the distance from the camera to the AR marker using solvePnP\n",
    "    # 3D coordinates\n",
    "    corner_points_3d = np.array([[-marker_size/2, -marker_size/2, 0],\n",
    "                                 [marker_size/2, -marker_size/2, 0],\n",
    "                                 [marker_size/2, marker_size/2, 0],\n",
    "                                 [-marker_size/2, marker_size/2, 0]], dtype=np.float32)\n",
    "    # 2D coordinates\n",
    "    corner_points_2d = np.array([[corner_pos[0], corner_pos[1]],\n",
    "                                 [corner_pos[2], corner_pos[3]],\n",
    "                                 [corner_pos[4], corner_pos[5]],\n",
    "                                 [corner_pos[6], corner_pos[7]]], dtype=np.float32)\n",
    "\n",
    "    # Calculate the distance from the camera to the AR marker using solvePnP\n",
    "    _, _, translation_vector = cv2.solvePnP(corner_points_3d, corner_points_2d, camera_matrix, dist_coeffs)\n",
    "\n",
    "    # Calculate the distance from the camera to the AR marker\n",
    "    depth = translation_vector[2][0]\n",
    "\n",
    "    return corner_pos, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Setting Camera Parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera settings\n",
    "fov = 60\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "center_x = image_width / 2\n",
    "center_y = image_height / 2\n",
    "aspect = image_width / image_height\n",
    "near = 0.05\n",
    "far = 10\n",
    "projection_matrix = pybullet.computeProjectionMatrixFOV(fov, aspect, near, far)\n",
    "\n",
    "# Calculate the focal length\n",
    "fov_rad = np.deg2rad(fov)\n",
    "f = image_height / (2 * np.tan(fov_rad / 2))\n",
    "\n",
    "# Camera intrinsic parameters\n",
    "camera_matrix = np.array([[f, 0, image_width/2],\n",
    "                         [0, f, image_height/2],\n",
    "                         [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# Distortion coefficients (assuming no distortion here)\n",
    "dist_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Setting Parameters for AR Markers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_size = 0.1 # Length of one side of the AR marker (meters)\n",
    "\n",
    "# Define the dictionary for the AR marker to be detected\n",
    "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "parameters = cv2.aruco.DetectorParameters_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Setting Parameters for the Robot Arm](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each link\n",
    "ARM_ORIGIN_X_WORLD = 0.0 # x-coordinate of the arm's origin in the world coordinate system\n",
    "ARM_ORIGIN_Y_WORLD = 0.0 # y-coordinate of the arm's origin in the world coordinate system\n",
    "ARM_ORIGIN_Z_WORLD = 0.8 # z-coordinate of the arm's origin in the world coordinate system (length of base_link in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK1_LENGTH = 0.3 # Length of link1 (z-direction length of link1 in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK2_LENGTH = 0.5 # Length of link2 (z-direction length of link2 in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK3_LENGTH = 0.5 # Length of link3 (z-direction length of link3 in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK4_LENGTH = 0.1 # Length of link4 (z-direction length of link4 in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK5_LENGTH = 0.1 # Length of link5 (z-direction length of link5 in \"simple6d_arm_with_gripper.urdf\")\n",
    "LINK6_LENGTH = 0.15 # Length of link6 (z-direction length of link6 in \"simple6d_arm_with_gripper.urdf\")\n",
    "CAMERA_Z_SIZE = 0.01 # Size of the camera in the z-direction\n",
    "CAMERA_X_OFFSET = 0.08 # x-direction offset from the origin of the link6 coordinate system to the origin of the camera coordinate system\n",
    "\n",
    "# Changing these values will change the results #############\n",
    "# Define the initial angles of each link\n",
    "link1_angle_deg = -90\n",
    "link2_angle_deg = 55\n",
    "link3_angle_deg = 35\n",
    "link4_angle_deg = 0\n",
    "link5_angle_deg = 0\n",
    "link6_angle_deg = 0\n",
    "feature_param = 10\n",
    "\n",
    "# Target positions of the 4 features on the image\n",
    "goal1_u = center_x - 50\n",
    "goal1_v = center_y - 50\n",
    "goal2_u = center_x + 50\n",
    "goal2_v = center_y - 50\n",
    "goal3_u = center_x + 50\n",
    "goal3_v = center_y + 50\n",
    "goal4_u = center_x - 50\n",
    "goal4_v = center_y + 50\n",
    "##############################################\n",
    "\n",
    "# Convert degrees to radians\n",
    "link1_angle_rad  = np.deg2rad(link1_angle_deg)\n",
    "link2_angle_rad  = np.deg2rad(link2_angle_deg)\n",
    "link3_angle_rad  = np.deg2rad(link3_angle_deg)\n",
    "link4_angle_rad  = np.deg2rad(link4_angle_deg)\n",
    "link5_angle_rad  = np.deg2rad(link5_angle_deg)\n",
    "link6_angle_rad  = np.deg2rad(link6_angle_deg)\n",
    "\n",
    "# List the target positions of the 4 features\n",
    "goal_feature_points = np.array([goal1_u, goal1_v, goal2_u, goal2_v, goal3_u, goal3_v, goal4_u, goal4_v])\n",
    "\n",
    "# Indices of each joint of the robot\n",
    "LINK1_JOINT_IDX = 0\n",
    "LINK2_JOINT_IDX = 1\n",
    "LINK3_JOINT_IDX = 2\n",
    "LINK4_JOINT_IDX = 3\n",
    "LINK5_JOINT_IDX = 4\n",
    "LINK6_JOINT_IDX = 5\n",
    "CAMERA_IDX = 6\n",
    "CAMERA_TARGET_IDX = 7\n",
    "\n",
    "# Initialize the angles of each joint of the robot\n",
    "pybullet.resetJointState(arm_id, LINK1_JOINT_IDX, link1_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK2_JOINT_IDX, link2_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK3_JOINT_IDX, link3_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK4_JOINT_IDX, link4_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK5_JOINT_IDX, link5_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK6_JOINT_IDX, link6_angle_rad )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Executing Visual Servoing](#toc0_)\n",
    "\n",
    "You can change the position and orientation of the box with the AR marker attached by adjusting the values of the sliders on the right.\n",
    "\n",
    "Note that with the default robot posture and box position, changes in the \"x, roll, yaw\" directions may not work properly unless adjusted carefully.  \n",
    "Therefore, it is recommended to start with changes in the \"y, z, pitch\" directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Global variables (variables used in the bonus chapter are defined as global variables)\n",
    "current_feature_points = None # Current position of feature points\n",
    "rgb_img = None # RGB image obtained from the camera\n",
    "\n",
    "def VisualServo():\n",
    "    global current_feature_points\n",
    "    global rgb_img\n",
    "\n",
    "    while True:\n",
    "        # Set the \"box with AR marker attached\" to the position and orientation set by the slider\n",
    "        obj_x = pybullet.readUserDebugParameter(0)\n",
    "        obj_y = pybullet.readUserDebugParameter(1)\n",
    "        obj_z = pybullet.readUserDebugParameter(2)\n",
    "        obj_roll = pybullet.readUserDebugParameter(3)\n",
    "        obj_pitch = pybullet.readUserDebugParameter(4)\n",
    "        obj_yaw = pybullet.readUserDebugParameter(5)\n",
    "        pybullet.resetBasePositionAndOrientation(box_bid, [obj_x, obj_y, obj_z], pybullet.getQuaternionFromEuler([obj_roll, obj_pitch, obj_yaw]))\n",
    "\n",
    "        # Get joint angles\n",
    "        q1 = pybullet.getJointState(arm_id, LINK1_JOINT_IDX)[0]\n",
    "        q2 = pybullet.getJointState(arm_id, LINK2_JOINT_IDX)[0]\n",
    "        q3 = pybullet.getJointState(arm_id, LINK3_JOINT_IDX)[0]\n",
    "        q4 = pybullet.getJointState(arm_id, LINK4_JOINT_IDX)[0]\n",
    "        q5 = pybullet.getJointState(arm_id, LINK5_JOINT_IDX)[0]\n",
    "        q6 = pybullet.getJointState(arm_id, LINK6_JOINT_IDX)[0]\n",
    "\n",
    "        # Set the camera's up vector according to the pose of the camera at the robot arm's end-effector\n",
    "        camera_up_vector = np.array([0, -1, 0]) # Default camera up vector\n",
    "        RW1 = Rz(theta=0)@Ry(theta=0)@Rx(theta=0) # World coordinate system -> link1 coordinate system\n",
    "        R12 = Rz(theta=q1) # link1 coordinate system -> link2 coordinate system\n",
    "        R23 = Ry(theta=q2) # link2 coordinate system -> link3 coordinate system\n",
    "        R34 = Ry(theta=q3) # link3 coordinate system -> link4 coordinate system\n",
    "        R45 = Rz(theta=q4) # link4 coordinate system -> link5 coordinate system\n",
    "        R56 = Ry(theta=q5) # link5 coordinate system -> link6 coordinate system\n",
    "        R6C = Rz(theta=q6-np.pi/2) # link6 coordinate system -> camera coordinate system (rotate the camera by -90 degrees to get the correct image orientation)\n",
    "        RWC = RW1@R12@R23@R34@R45@R56@R6C # World coordinate system -> camera coordinate system\n",
    "        rotate_camera_up_vector = RWC@camera_up_vector # Camera up vector in the camera coordinate system\n",
    "\n",
    "        # Get camera image\n",
    "        camera_link_pose = pybullet.getLinkState(arm_id, CAMERA_IDX)[0] # Position of the camera link at the end-effector\n",
    "        cameraTargetLinkPose = pybullet.getLinkState(arm_id, CAMERA_TARGET_IDX)[0] # Position of a virtual link set slightly ahead of the camera link\n",
    "        view_matrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]],cameraTargetPosition=[cameraTargetLinkPose[0], cameraTargetLinkPose[1], cameraTargetLinkPose[2]],cameraUpVector=rotate_camera_up_vector)\n",
    "        _, _, rgb_img, _, _ = pybullet.getCameraImage(\n",
    "            width=image_width,\n",
    "            height=image_height,\n",
    "            viewMatrix=view_matrix,\n",
    "            projectionMatrix=projection_matrix,\n",
    "            renderer=pybullet.ER_BULLET_HARDWARE_OPENGL\n",
    "        )\n",
    "\n",
    "        # Get the position of the four feature points in the image coordinate system (pixel position) and the distance from the camera to the object [m]\n",
    "        current_feature_points, z = detect_ar_marker_corner_pos_and_depth(marker_size, aruco_dict, parameters, rgb_img, camera_matrix, dist_coeffs)\n",
    "\n",
    "        # If feature points cannot be detected, proceed to the next loop\n",
    "        if current_feature_points is None:\n",
    "            continue\n",
    "\n",
    "        # Image Jacobian matrix\n",
    "        Ji = make_4features_image_jacobian_matrix(current_feature_points, z, f)\n",
    "        Ji_inv = np.linalg.pinv(Ji) # Inverse of the image Jacobian matrix\n",
    "\n",
    "        # Calculate the position of each link's coordinate system from the world coordinate system (used for calculating the basic Jacobian matrix)\n",
    "        TW1 = Hz(theta=0)@Hy(theta=0)@Hx(theta=0)@Hp(x=ARM_ORIGIN_X_WORLD, y=ARM_ORIGIN_Y_WORLD, z=ARM_ORIGIN_Z_WORLD) # World coordinate system -> link1 coordinate system\n",
    "        T12 = Hz(theta=q1)@Hp(x=0, y=0, z=LINK1_LENGTH) # link1 coordinate system -> link2 coordinate system\n",
    "        T23 = Hy(theta=q2)@Hp(x=0, y=0, z=LINK2_LENGTH) # link2 coordinate system -> link3 coordinate system\n",
    "        T34 = Hy(theta=q3)@Hp(x=0, y=0, z=LINK3_LENGTH) # link3 coordinate system -> link4 coordinate system\n",
    "        T45 = Hz(theta=q4)@Hp(x=0, y=0, z=LINK4_LENGTH) # link4 coordinate system -> link5 coordinate system\n",
    "        T56 = Hy(theta=q5)@Hp(x=0, y=0, z=LINK5_LENGTH) # link5 coordinate system -> link6 coordinate system\n",
    "        T6C = Hz(theta=q6-np.pi/2)@Hp(x=CAMERA_X_OFFSET, y=0.0, z=LINK6_LENGTH-CAMERA_Z_SIZE/2) # link6 coordinate system -> camera coordinate system\n",
    "        origin_pos = np.array([0, 0, 0, 1]) # Origin position\n",
    "        origin_link1_world = TW1@origin_pos                         # Origin position of link1 coordinate system from the world coordinate system\n",
    "        origin_link2_world = TW1@T12@origin_pos                     # Origin position of link2 coordinate system from the world coordinate system\n",
    "        origin_link3_world = TW1@T12@T23@origin_pos                 # Origin position of link3 coordinate system from the world coordinate system\n",
    "        origin_link4_world = TW1@T12@T23@T34@origin_pos             # Origin position of link4 coordinate system from the world coordinate system\n",
    "        origin_link5_world = TW1@T12@T23@T34@T45@origin_pos         # Origin position of link5 coordinate system from the world coordinate system\n",
    "        origin_link6_world = TW1@T12@T23@T34@T45@T56@origin_pos     # Origin position of link6 coordinate system from the world coordinate system\n",
    "        origin_linkC_world = TW1@T12@T23@T34@T45@T56@T6C@origin_pos # Origin position of camera coordinate system from the world coordinate system\n",
    "\n",
    "        # From here, the main process of visual servoing ##########################################################\n",
    "        # Vector from \"current feature point position\" to \"target feature point position\"\n",
    "        feature_current_to_goal = goal_feature_points - current_feature_points\n",
    "\n",
    "        # Small change amount towards the direction of \"current feature point position\" to \"target feature point position\"\n",
    "        delta_feature_points = feature_param * feature_current_to_goal\n",
    "\n",
    "        # Using the inverse of the image Jacobian matrix Ji_inv, calculate the small change amount of the camera (Δx, Δy, Δz, Δroll, Δpitch, Δyaw) camera_delta_p from the small change amount of the feature points delta_feature_points\n",
    "        # (How to move the camera so that the feature points move to the target position)\n",
    "        delta_p_camera = Ji_inv @ delta_feature_points\n",
    "\n",
    "        # Diagonal matrix with R on the diagonal\n",
    "        size = 6\n",
    "        rotate_diagonal_matrix = np.zeros((size, size))\n",
    "        for i in range(0, size, 3):\n",
    "            rotate_diagonal_matrix[i:i+3, i:i+3] = RWC\n",
    "\n",
    "        # Using the rotation matrix rotate_diagonal_matrix, convert the small change amount of the camera delta_p_camera based on the camera coordinate system to the small change amount of the camera delta_p_world based on the world coordinate system\n",
    "        delta_p_world = rotate_diagonal_matrix @ delta_p_camera\n",
    "\n",
    "        # Basic Jacobian matrix\n",
    "        Jv = make_6d_jacobian_matrix(R=[R12, R23, R34, R45, R56, R6C],\n",
    "                                     o1=origin_link1_world[0:3], \n",
    "                                     o2=origin_link2_world[0:3], \n",
    "                                     o3=origin_link3_world[0:3], \n",
    "                                     o4=origin_link4_world[0:3], \n",
    "                                     o5=origin_link5_world[0:3], \n",
    "                                     o6=origin_link6_world[0:3], \n",
    "                                     oc=origin_linkC_world[0:3])\n",
    "        Jv_inv = np.linalg.pinv(Jv) # Inverse of the basic Jacobian matrix\n",
    "\n",
    "        # Using the inverse of the basic Jacobian matrix Jv_inv, calculate the joint angular velocities delta_q from the small change amount of the camera delta_p_world based on the world coordinate system\n",
    "        # (How to move each joint of the robot arm so that the camera moves to the target position and orientation)\n",
    "        delta_q = Jv_inv @ delta_p_world\n",
    "\n",
    "        # Set the calculated angular velocities to each joint\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK1_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[0])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK2_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[1])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK3_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[2])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK4_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[3])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK5_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[4])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK6_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[5])\n",
    "\n",
    "        # Advance by one time step\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(time_step)\n",
    "\n",
    "# Start the thread\n",
    "thread = threading.Thread(target=VisualServo)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Bonus: Drawing Feature Point Positions](#toc0_)\n",
    "\n",
    "By running the code below, the \"goal feature point positions\" and \"current feature point positions\" will be drawn.\n",
    "\n",
    "You can specifically check how the feature points are moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "plt.ion()  # Enable interactive mode\n",
    "fig, ax = plt.subplots()\n",
    "goal_plot, = ax.plot([], [], 'ro', label=\"Goal Points\")\n",
    "current_plot, = ax.plot([], [], 'bo', label=\"Current Points\")\n",
    "ax.set_xlim(0, image_width)\n",
    "ax.set_ylim(0, image_height)\n",
    "ax.invert_yaxis()  # Invert y-axis to match the image coordinate system\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Main loop to draw the image obtained from the camera\n",
    "initDraw = True\n",
    "while True:\n",
    "    # Draw rgb_img and plot goal_feature_points and current_feature_points\n",
    "    try:\n",
    "        if goal_feature_points is not None and current_feature_points is not None:\n",
    "            if initDraw:\n",
    "                img_plot = ax.imshow(rgb_img)\n",
    "                initDraw = False\n",
    "            else:\n",
    "                img_plot.set_data(rgb_img)\n",
    "            goal_plot.set_xdata([goal1_u, goal2_u, goal3_u, goal4_u])\n",
    "            goal_plot.set_ydata([goal1_v, goal2_v, goal3_v, goal4_v])\n",
    "            current_plot.set_xdata([current_feature_points[0], current_feature_points[2], current_feature_points[4], current_feature_points[6]])\n",
    "            current_plot.set_ydata([current_feature_points[1], current_feature_points[3], current_feature_points[5], current_feature_points[7]])\n",
    "            plt.pause(0.001)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
