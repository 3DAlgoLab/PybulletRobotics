{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Robot Arm Eye to Hand Estimate Object Pose](#toc1_)    \n",
    "- [Starting pybullet](#toc2_)    \n",
    "- [Initial Setup for pybullet](#toc3_)    \n",
    "- [Generating the Robot Arm](#toc4_)    \n",
    "- [Defining the Functions to be Used](#toc5_)    \n",
    "- [Generating Colored Objects](#toc6_)    \n",
    "- [Generating the Camera](#toc7_)    \n",
    "- [Setting Camera Parameters](#toc8_)    \n",
    "- [Capturing Images](#toc9_)    \n",
    "- [Estimating the Position of the Colored Object](#toc10_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Robot Arm Eye to Hand Estimate Object Pose](#toc0_)\n",
    "\n",
    "In this notebook, we will generate a 6-axis robot arm and introduce a method to estimate the position of a specified colored object using a \"fixed camera\".\n",
    "\n",
    "(Note: Since we are estimating the position from a fixed camera, we will not control the arm.)\n",
    "\n",
    "(For a manual summarizing the functions available in pybullet, refer to [this link](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Starting pybullet](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import pybullet\n",
    "import pybullet_data\n",
    "physics_client = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Initial Setup for pybullet](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # Reset the simulation space\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # Add paths to necessary data for pybullet\n",
    "pybullet.setGravity(0.0, 0.0, -9.8) # Set gravity as on Earth\n",
    "time_step = 1./240.\n",
    "pybullet.setTimeStep(time_step) # Set the time elapsed per step\n",
    "\n",
    "# Load the floor\n",
    "plane_id = pybullet.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Set the camera position and other parameters in GUI mode\n",
    "camera_distance = 3.5\n",
    "camera_yaw = 180.0 # deg\n",
    "camera_pitch = -40 # deg\n",
    "cameraTargetPosition = [0, 0.5, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, cameraTargetPosition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Generating the Robot Arm](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: target_position_vertual_link\n"
     ]
    }
   ],
   "source": [
    "# Load the robot\n",
    "arm_start_pos = [0, 0, 0.0]  # Set the initial position (x, y, z)\n",
    "arm_start_orientation = pybullet.getQuaternionFromEuler([0, 0, 0])  # Set the initial orientation (roll, pitch, yaw)\n",
    "arm_id = pybullet.loadURDF(\"../urdf/simple6d_arm_with_gripper.urdf\", arm_start_pos, arm_start_orientation, useFixedBase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Defining the Functions to be Used](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_color_obj_pose(target_rgb, rgb_img, depth_img):\n",
    "    \"\"\"\n",
    "    Function to obtain the center position, depth, and orientation of the first detected colored object\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_rgb : list\n",
    "        RGB of the color to be detected\n",
    "    rgb_img : numpy.ndarray\n",
    "        Camera image (RGB)\n",
    "    depth_img : numpy.ndarray\n",
    "        Camera image (Depth)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj_pose : numpy.ndarray\n",
    "        Position and orientation of the colored object (x, y, z, roll, pitch, yaw)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the camera image to HSV format\n",
    "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Convert RGB to HSV\n",
    "    target_hsv = cv2.cvtColor(np.uint8([[target_rgb]]), cv2.COLOR_RGB2HSV)[0][0]\n",
    "\n",
    "    # Specify the range of the color to be detected\n",
    "    lower = np.array([target_hsv[0]-10, 50, 50])\n",
    "    upper = np.array([target_hsv[0]+10, 255, 255])\n",
    "\n",
    "    # Extract only the specified color\n",
    "    mask = cv2.inRange(hsv_img, lower, upper)\n",
    "\n",
    "    # Extract contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get the contour with the largest area\n",
    "    max_area = 0\n",
    "    max_area_contour = None\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_area_contour = contour\n",
    "\n",
    "    # If no contour is found\n",
    "    if max_area_contour is None:\n",
    "        return None\n",
    "    \n",
    "    # Get the center position of the contour\n",
    "    M = cv2.moments(max_area_contour)\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "\n",
    "    # Get the depth at the center position of the contour\n",
    "    depth = depth_img[cy, cx]\n",
    "\n",
    "    pos = [cx, cy, depth]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def Rx(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the x-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the x-axis\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the y-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the y-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix around the z-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Rotation matrix around the z-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "\n",
    "def Hx(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the x-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the x-axis\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta), 0],\n",
    "                     [0, np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hy(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the y-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the y-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta), 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hz(theta):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix around the z-axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        Rotation angle [rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix around the z-axis\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0, 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hp(x, y, z):\n",
    "    \"\"\"\n",
    "    Calculate the homogeneous transformation matrix for translation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Translation in the x direction\n",
    "    y : float\n",
    "        Translation in the y direction\n",
    "    z : float\n",
    "        Translation in the z direction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Homogeneous transformation matrix for translation\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Generating Colored Objects](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change this to get different results (make sure it fits within the field of view of the simpleCamera) ####\n",
    "color_box_pos = [-2.0, 1.5, 0.05] # Set the initial position (x, y, z) of the colored object\n",
    "#################################################################################\n",
    "\n",
    "color_box_id = pybullet.loadURDF(\"../urdf/simple_box.urdf\", color_box_pos, pybullet.getQuaternionFromEuler([0.0, 0.0, 0.0]), globalScaling=0.1, useFixedBase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Generating the Camera](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: base_link\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: target_position_vertual_link\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# You can change this to get different results (make sure the colorBox fits within the field of view of the simpleCamera) ####\n",
    "SIMPLE_CAMERA_X = 0.0\n",
    "SIMPLE_CAMERA_Y = 0.0\n",
    "SIMPLE_CAMERA_Z = 4.0\n",
    "SIMPLE_CAMERA_ROLL = 0.0\n",
    "SIMPLE_CAMERA_PITCH = 0.0\n",
    "SIMPLE_CAMERA_YAW = 0.0\n",
    "#################################################################################\n",
    "\n",
    "SIMPLE_CAMERA_ROLL = math.radians(180.0 + SIMPLE_CAMERA_ROLL)\n",
    "SIMPLE_CAMERA_PITCH = math.radians(0.0 + SIMPLE_CAMERA_PITCH)\n",
    "SIMPLE_CAMERA_YAW = math.radians(0.0 + SIMPLE_CAMERA_YAW)\n",
    "simple_camera_id = pybullet.loadURDF(\"../urdf/simple_camera.urdf\", [SIMPLE_CAMERA_X, SIMPLE_CAMERA_Y, SIMPLE_CAMERA_Z], pybullet.getQuaternionFromEuler([SIMPLE_CAMERA_ROLL, SIMPLE_CAMERA_PITCH, SIMPLE_CAMERA_YAW]), useFixedBase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Setting Camera Parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera settings\n",
    "fov = 60\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "aspect = image_width / image_height\n",
    "near = 0.05\n",
    "far = 5\n",
    "projection_matrix = pybullet.computeProjectionMatrixFOV(fov, aspect, near, far)\n",
    "\n",
    "# Calculate the focal length\n",
    "fov_rad = np.deg2rad(fov)\n",
    "f = (image_height / 2) / np.tan(fov_rad / 2)\n",
    "\n",
    "# Camera intrinsic parameters\n",
    "camera_matrix = np.array([[f, 0, image_width//2],\n",
    "                         [0, f, image_height//2],\n",
    "                         [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# Distortion coefficients (assuming no distortion here)\n",
    "dist_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Capturing Images](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_CAMERA_LINK_IDX = 0\n",
    "SIMPLE_CAMERA_TARGET_LINK_IDX = 1\n",
    "\n",
    "# Get the position of the camera\n",
    "camera_link_pose = pybullet.getLinkState(simple_camera_id, SIMPLE_CAMERA_LINK_IDX)[0] # Position of the camera link at the end of the arm\n",
    "camera_target_link_pose = pybullet.getLinkState(simple_camera_id, SIMPLE_CAMERA_TARGET_LINK_IDX)[0] # Position of a virtual link set just in front of the camera link\n",
    "camera_link_orientation = pybullet.getEulerFromQuaternion(pybullet.getLinkState(simple_camera_id, SIMPLE_CAMERA_LINK_IDX)[1]) # Orientation of the camera link at the end of the arm\n",
    "\n",
    "# Rotate the up vector of the camera according to the camera's orientation\n",
    "camera_up_vector = np.array([0, -1, 0]) # Default up vector of the camera\n",
    "R = Rz(SIMPLE_CAMERA_YAW) @ Ry(SIMPLE_CAMERA_PITCH) @ Rx(SIMPLE_CAMERA_ROLL)\n",
    "rotate_camera_up_vector = R @ camera_up_vector\n",
    "\n",
    "# Compute the view matrix of the camera\n",
    "view_matrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]], cameraTargetPosition=[camera_target_link_pose[0], camera_target_link_pose[1], camera_target_link_pose[2]], cameraUpVector=[rotate_camera_up_vector[0], rotate_camera_up_vector[1], rotate_camera_up_vector[2]])\n",
    "\n",
    "# Get the camera image\n",
    "_, _, rgb_img, depth_img, _ = pybullet.getCameraImage(\n",
    "    width=image_width,\n",
    "    height=image_height,\n",
    "    viewMatrix=view_matrix,\n",
    "    projectionMatrix=projection_matrix,\n",
    "    renderer=pybullet.ER_BULLET_HARDWARE_OPENGL\n",
    ")\n",
    "\n",
    "# Get the position of the object of the specified color\n",
    "detect_color_rgb = [0, 0, 255] # Specify the color to be detected (red in this case)\n",
    "pos = detect_color_obj_pose(detect_color_rgb, rgb_img, depth_img)\n",
    "pixel_x = pos[0] # x-coordinate (position on the image)\n",
    "pixel_y = pos[1] # y-coordinate (position on the image)\n",
    "normal_z = pos[2] # z-coordinate (depth normalized to 0-1)\n",
    "z = far * near / (far - (far - near) * normal_z) # Convert depth normalized to 0-1 to distance [m]\n",
    "x = (pixel_x - image_width // 2) * z / f # Convert pixelX to distance [m]\n",
    "y = (pixel_y - image_height // 2) * z / f # Convert pixelY to distance [m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Estimating the Position of the Colored Object](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color box obj pose:  -2.0 1.5 0.05\n",
      "estimate color obj pose -1.9903058737195025 1.5078074800905317 0.10001191576652735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the position of the colored object in the world coordinate system\n",
    "CAMERA_POS = np.array([SIMPLE_CAMERA_X, \n",
    "                      SIMPLE_CAMERA_Y, \n",
    "                      SIMPLE_CAMERA_Z])\n",
    "color_obj_pos_camera = np.array([x, y, z]) # Position of the object in the camera coordinate system\n",
    "color_obj_pos_world = CAMERA_POS + R @ color_obj_pos_camera # Position of the object in the world coordinate system\n",
    "\n",
    "# Get the position of the box\n",
    "box_pos, box_orn = pybullet.getBasePositionAndOrientation(color_box_id)\n",
    "print(\"color box obj pose: \", box_pos[0], box_pos[1], box_pos[2])\n",
    "print(\"estimate color obj pose\", color_obj_pos_world[0], color_obj_pos_world[1], color_obj_pos_world[2])\n",
    "\n",
    "# Draw the position of the object on the Pybullet screen\n",
    "pybullet.addUserDebugLine(CAMERA_POS[:3], color_obj_pos_world[:3], lineColorRGB=[1, 0, 0], lineWidth=5)\n",
    "\n",
    "# Draw the position of the box\n",
    "pybullet.addUserDebugText(f\"true box pose ({box_pos[0]:.3f}, {box_pos[1]:.3f}, {box_pos[2]:.3f})\", [4.0, 0.5, 0], textColorRGB=[1, 0, 0], textSize=1.3)\n",
    "pybullet.addUserDebugText(f\"eye to hand, estimate box pose ({color_obj_pos_world[0]:.3f}, {color_obj_pos_world[1]:.3f}, {color_obj_pos_world[2]:.3f})\", [4.0, 1.0, 0.0], textColorRGB=[1, 0, 0], textSize=1.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
