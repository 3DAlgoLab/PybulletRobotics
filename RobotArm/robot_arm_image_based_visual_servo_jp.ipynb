{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [image based visual servo(IBVS)](#toc1_)    \n",
    "- [pybulletの起動](#toc2_)    \n",
    "- [pybulletの初期設定](#toc3_)    \n",
    "- [ロボットアームの生成](#toc4_)    \n",
    "- [ARマーカが張り付けられたボックスを生成](#toc5_)    \n",
    "- [光源位置の変更](#toc6_)    \n",
    "- [使用関数の定義](#toc7_)    \n",
    "- [カメラのパラメータ設定](#toc8_)    \n",
    "- [ARマーカに関するパラメータの設定](#toc9_)    \n",
    "- [ロボットアームのパラメータなどの設定](#toc10_)    \n",
    "- [ビジュアルサーボの実行](#toc11_)    \n",
    "- [（おまけ）特徴点位置の描画](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[image based visual servo(IBVS)](#toc0_)\n",
    "\n",
    "本notebookでは6軸のロボットアームを用いてimage based visual servoを行う方法を紹介します。\n",
    "\n",
    "ビジュアルサーボ参考：https://github.com/RiddhimanRaut/Ur5_Visual_Servoing/blob/master/ur5_control_nodes/src/vs_ur5.py\n",
    "\n",
    "\n",
    "（pybulletで使用可能な関数がまとめられたマニュアルについては[こちら](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf)を参照してください。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[pybulletの起動](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import pybullet\n",
    "import pybullet_data\n",
    "physics_client = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[pybulletの初期設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # シミュレーション空間をリセット\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # pybulletに必要なデータへのパスを追加\n",
    "# pybullet.setGravity(0.0, 0.0, -9.8) # 地球上における重力に設定（今回は、簡単のため重力は考慮しないこととする）\n",
    "time_step = 1./240.\n",
    "pybullet.setTimeStep(time_step) # 1stepあたりに経過する時間の設定\n",
    "\n",
    "#床の読み込み\n",
    "plane_id = pybullet.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# GUIモードの際のカメラの位置などを設定\n",
    "camera_distance = 2.0\n",
    "camera_yaw = 0.0 # deg\n",
    "camera_pitch = -20 # deg\n",
    "camera_target_position = [0.0, 0.0, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[ロボットアームの生成](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: target_position_vertual_link\n"
     ]
    }
   ],
   "source": [
    "# ロボットの読み込み\n",
    "arm_start_pos = [0, 0, 0]  # 初期位置(x,y,z)を設定\n",
    "arm_start_orientation = pybullet.getQuaternionFromEuler([0,0,0])  # 初期姿勢(roll, pitch, yaw)を設定\n",
    "arm_id = pybullet.loadURDF(\"../urdf/simple6d_arm_with_force_sensor.urdf\",arm_start_pos, arm_start_orientation, useFixedBase=True) # ロボットが倒れないように、useFixedBase=Trueでルートのリンクを固定\n",
    "\n",
    "# GUIモードの際のカメラの位置などを設定\n",
    "camera_distance = 1.5\n",
    "camera_yaw = 50.0 # deg\n",
    "camera_pitch = -40 # deg\n",
    "camera_target_position = [0.0, 0.0, 1.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[ARマーカが張り付けられたボックスを生成](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一面にARマーカが描かれたボックスの生成\n",
    "box_bid = pybullet.loadURDF(\"../urdf/ar_marker_box.urdf\", [0, 0, 0.5], useFixedBase=True)\n",
    "\n",
    "# textureを設定(urdfファイルで指定したものと、同じものを指定)\n",
    "texture_id = pybullet.loadTexture(\"../texture/ar_marker_box.png\")\n",
    "pybullet.changeVisualShape(box_bid, -1, textureUniqueId=texture_id)\n",
    "\n",
    "# ARマーカが張り付けられたボックスの位置を調整するスライダーを設定 \n",
    "pybullet.addUserDebugParameter(\"obj_x\", -4, 4, 0.0)\n",
    "pybullet.addUserDebugParameter(\"obj_y\", -2, 0, -1.5)\n",
    "pybullet.addUserDebugParameter(\"obj_z\", -4, 8, 1.3)\n",
    "pybullet.addUserDebugParameter(\"obj_roll\", -3.14, 3.14, 0.0)\n",
    "pybullet.addUserDebugParameter(\"obj_pitch\", -3.14, 3.14, 0)\n",
    "pybullet.addUserDebugParameter(\"obj_yaw\", -3.14, 3.14, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[光源位置の変更](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デフォルトの光源位置だと、ARマーカが認識されにくいので、光源位置を変更\n",
    "pybullet.configureDebugVisualizer(lightPosition=[0, 0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[使用関数の定義](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def Rx(theta):\n",
    "    \"\"\"\n",
    "    x軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        x軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    \"\"\"\n",
    "    y軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        y軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    \"\"\"\n",
    "    z軸周りの回転行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        z軸周りの回転行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "\n",
    "def Hx(theta):\n",
    "    \"\"\"\n",
    "    x軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        x軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta), 0],\n",
    "                     [0, np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hy(theta):\n",
    "    \"\"\"\n",
    "    y軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        y軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta), 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hz(theta):\n",
    "    \"\"\"\n",
    "    z軸回りの同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        回転角度[rad]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        z軸回りの同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0, 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "def Hp(x, y, z):\n",
    "    \"\"\"\n",
    "    平行移動の同次変換行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        x方向の移動量\n",
    "    y : float\n",
    "        y方向の移動量\n",
    "    z : float\n",
    "        z方向の移動量\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        平行移動の同次変換行列\n",
    "    \"\"\"\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]])\n",
    "                     \n",
    "def make_6d_jacobian_matrix(R, o1, o2, o3, o4, o5, o6, oc):\n",
    "    \"\"\"\n",
    "    6軸ロボットアームの基礎ヤコビ行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    R : numpy.ndarray\n",
    "        ロボットアームの姿勢行列の各座標系間の回転行列\n",
    "    o1 : numpy.ndarray\n",
    "        リンク1の原点\n",
    "    o2 : numpy.ndarray\n",
    "        リンク2の原点\n",
    "    o3 : numpy.ndarray\n",
    "        リンク3の原点\n",
    "    o4 : numpy.ndarray\n",
    "        リンク4の原点\n",
    "    o5 : numpy.ndarray\n",
    "        リンク5の原点\n",
    "    o6 : numpy.ndarray\n",
    "        リンク6の原点\n",
    "    oc : numpy.ndarray\n",
    "        カメラの原点\n",
    "   \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Jv : numpy.ndarray\n",
    "         基礎ヤコビ行列\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    R12 = R[0]\n",
    "    R23 = R[1]\n",
    "    R34 = R[2]\n",
    "    R45 = R[3]\n",
    "    R56 = R[4]\n",
    "    R6C = R[5]\n",
    "    ex = np.array([1, 0, 0])\n",
    "    ey = np.array([0, 1, 0])\n",
    "    ez = np.array([0, 0, 1])\n",
    "\n",
    "    # ヤコビ行列の各要素を計算\n",
    "    a1 = R12 @ ez\n",
    "    a2 = R12 @ R23 @ ey\n",
    "    a3 = R12 @ R23 @ R34 @ ey\n",
    "    a4 = R12 @ R23 @ R34 @ R45 @ ez\n",
    "    a5 = R12 @ R23 @ R34 @ R45 @ R56 @ ey\n",
    "    a6 = R12 @ R23 @ R34 @ R45 @ R56 @ R6C @ ez\n",
    "\n",
    "    # Ji = [[ai x (oc - oi)], \n",
    "    #       [ai]]\n",
    "    j1 = np.concatenate((np.cross(a1, oc-o1).reshape(3, 1), a1.reshape(3, 1)), axis=0)\n",
    "    j2 = np.concatenate((np.cross(a2, oc-o2).reshape(3, 1), a2.reshape(3, 1)), axis=0)\n",
    "    j3 = np.concatenate((np.cross(a3, oc-o3).reshape(3, 1), a3.reshape(3, 1)), axis=0)\n",
    "    j4 = np.concatenate((np.cross(a4, oc-o4).reshape(3, 1), a4.reshape(3, 1)), axis=0)\n",
    "    j5 = np.concatenate((np.cross(a5, oc-o5).reshape(3, 1), a5.reshape(3, 1)), axis=0)\n",
    "    j6 = np.concatenate((np.cross(a6, oc-o6).reshape(3, 1), a6.reshape(3, 1)), axis=0)\n",
    "\n",
    "    Jv = np.concatenate((j1, j2, j3, j4, j5, j6), axis=1)\n",
    "    return Jv\n",
    "\n",
    "def make_4features_image_jacobian_matrix(current_feature_points, camera_to_obj, f):\n",
    "    \"\"\"\n",
    "    4つの特徴点の画像ヤコビ行列を求める\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_feature_points : numpy.ndarray\n",
    "        現在の特徴点座標\n",
    "    camera_to_objs : float\n",
    "        カメラからオブジェクトまでの距離\n",
    "    f : float\n",
    "        カメラの焦点距離\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Ji : numpy.ndarray\n",
    "        画像ヤコビ行列\n",
    "    \"\"\"\n",
    "    # 4つの特徴点の画像ヤコビ行列\n",
    "    u1 = current_feature_points[0]\n",
    "    v1 = current_feature_points[1]\n",
    "    u2 = current_feature_points[2]\n",
    "    v2 = current_feature_points[3]\n",
    "    u3 = current_feature_points[4]\n",
    "    v3 = current_feature_points[5]\n",
    "    u4 = current_feature_points[6]\n",
    "    v4 = current_feature_points[7]\n",
    "\n",
    "    Ji = np.array([[-f*(1/camera_to_obj), 0, u1*(1/camera_to_obj), u1*v1/f, -(f+(1/f)*u1**2), v1],\n",
    "                   [0, -f*(1/camera_to_obj), v1*(1/camera_to_obj), f+(1/f)*v1**2, -(1/f)*u1*v1, -u1],\n",
    "                   [-f*(1/camera_to_obj), 0, u2*(1/camera_to_obj), u2*v2/f, -(f+(1/f)*u2**2), v2],\n",
    "                   [0, -f*(1/camera_to_obj), v2*(1/camera_to_obj), f+(1/f)*v2**2, -(1/f)*u2*v2, -u2],\n",
    "                   [-f*(1/camera_to_obj), 0, u3*(1/camera_to_obj), u3*v3/f, -(f+(1/f)*u3**2), v3],\n",
    "                   [0, -f*(1/camera_to_obj), v3*(1/camera_to_obj), f+(1/f)*v3**2, -(1/f)*u3*v3, -u3],\n",
    "                   [-f*(1/camera_to_obj), 0, u4*(1/camera_to_obj), u4*v4/f, -(f+(1/f)*u4**2), v4],\n",
    "                   [0, -f*(1/camera_to_obj), v4*(1/camera_to_obj), f+(1/f)*v4**2, -(1/f)*u4*v4, -u4]])\n",
    "    return Ji\n",
    "\n",
    "def detect_ar_marker_corner_pos_and_depth(marker_size, aruco_dict, parameters, rgb_img, camera_matrix, dist_coeffs):\n",
    "    \"\"\"\n",
    "    ARマーカの4隅の位置とカメラまでの距離を検出する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    marker_size : float\n",
    "        ARマーカの1辺の長さ（メートル）\n",
    "    aruco_dict : cv2.aruco.Dictionary\n",
    "        ARマーカの辞書\n",
    "    parameters : cv2.aruco.DetectorParameters\n",
    "        ARマーカ検出のパラメータ\n",
    "    rgb_img : numpy.ndarray\n",
    "        カメラ画像（RGB）\n",
    "    camera_matrix : numpy.ndarray\n",
    "        カメラの内部パラメータ行列\n",
    "    dist_coeffs : numpy.ndarray\n",
    "        歪み係数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corner_pos : numpy.ndarray\n",
    "        ARマーカの4つの角の位置(x1, y1, x2, y2, x3, y3, x4, y4)[pixel位置]\n",
    "    depth : float\n",
    "        カメラからARマーカまでの距離[m]\n",
    "    \"\"\"\n",
    "\n",
    "    # カメラ画像をグレースケールに変換\n",
    "    rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # ARマーカの検出\n",
    "    corners, ids, _ = cv2.aruco.detectMarkers(rgb_img, aruco_dict, parameters=parameters)\n",
    "\n",
    "    # 検出されたマーカがない場合はNoneを返す\n",
    "    if ids is None or len(ids) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    # 最初に検出されたマーカの4隅の位置を取得\n",
    "    marker_corners = corners[0][0]\n",
    "\n",
    "    # 各隅の位置と対応する深度を取得\n",
    "    corner_pos = np.zeros(8)\n",
    "    for i in range(4):\n",
    "        x, y = int(marker_corners[i][0]), int(marker_corners[i][1])\n",
    "        # depth = depthImg[y, x]\n",
    "        corner_pos[i*2] = x\n",
    "        corner_pos[i*2+1] = y\n",
    "\n",
    "    # aolvePnPでカメラからARマーカまでの距離を求める\n",
    "    # 3次元座標\n",
    "    marker_size = 0.1\n",
    "    corner_points_3d = np.array([[-marker_size/2, -marker_size/2, 0],\n",
    "                     [marker_size/2, -marker_size/2, 0],\n",
    "                     [marker_size/2, marker_size/2, 0],\n",
    "                     [-marker_size/2, marker_size/2, 0]], dtype=np.float32)\n",
    "    # 2次元座標\n",
    "    corner_points_2d = np.array([[corner_pos[0], corner_pos[1]],\n",
    "                             [corner_pos[2], corner_pos[3]],\n",
    "                             [corner_pos[4], corner_pos[5]],\n",
    "                             [corner_pos[6], corner_pos[7]],], dtype=np.float32)\n",
    "\n",
    "    # solvePnPでカメラからARマーカまでの距離を求める\n",
    "    _, _, translation_vector = cv2.solvePnP(corner_points_3d, corner_points_2d, camera_matrix, dist_coeffs)\n",
    "\n",
    "    # カメラからARマーカまでの距離を求める\n",
    "    depth = translation_vector[2][0]\n",
    "\n",
    "    return corner_pos, depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[カメラのパラメータ設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラ設定\n",
    "fov = 60\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "center_x = image_width / 2\n",
    "center_y = image_height / 2\n",
    "aspect = image_width / image_height\n",
    "near = 0.05\n",
    "far = 10\n",
    "projection_matrix = pybullet.computeProjectionMatrixFOV(fov, aspect, near, far)\n",
    "\n",
    "# 焦点距離を求める\n",
    "fov_rad = np.deg2rad(fov)\n",
    "f = image_height / (2 * np.tan(fov_rad / 2))\n",
    "\n",
    "# カメラの内部パラメータ\n",
    "camera_matrix = np.array([[f, 0, image_width/2],\n",
    "                         [0, f, image_height/2],\n",
    "                         [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# 歪み係数（ここでは、歪みがないと仮定）\n",
    "dist_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[ARマーカに関するパラメータの設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_size = 0.1 # ARマーカの1辺の長さ（メートル）\n",
    "\n",
    "# 検出するARマーカの辞書を定義\n",
    "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "parameters = cv2.aruco.DetectorParameters_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[ロボットアームのパラメータなどの設定](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各リンクの長さ\n",
    "ARM_ORIGIN_X_WORLD = 0.0 # ワールド座標系から見たアームの原点のx座標\n",
    "ARM_ORIGIN_Y_WORLD = 0.0 # ワールド座標系から見たアームの原点のy座標\n",
    "ARM_ORIGIN_Z_WORLD = 0.8 # ワールド座標系から見たアームの原点のz座標(「simple6d_arm_with_gripper.urdf」の base_link の z方向の長さ)\n",
    "LINK1_LENGTH = 0.3 # link1の長さ(「simple6d_arm_with_gripper.urdf」の link1 の z方向の長さ)\n",
    "LINK2_LENGTH = 0.5 # link2の長さ(「simple6d_arm_with_gripper.urdf」の link2 の z方向の長さ)\n",
    "LINK3_LENGTH = 0.5 # link3の長さ(「simple6d_arm_with_gripper.urdf」の link3 の z方向の長さ)\n",
    "LINK4_LENGTH = 0.1 # link4の長さ(「simple6d_arm_with_gripper.urdf」の link4 の z方向の長さ)\n",
    "LINK5_LENGTH = 0.1 # link5の長さ(「simple6d_arm_with_gripper.urdf」の link5 の z方向の長さ)\n",
    "LINK6_LENGTH = 0.15 # link6の長さ(「simple6d_arm_with_gripper.urdf」の link6 の z方向の長さ)\n",
    "CAMERA_Z_SIZE = 0.01 # カメラのz方向のサイズ\n",
    "CAMERA_X_OFFSET = 0.08 # link6座標系の原点からカメラ座標系の原点までのx方向のオフセット\n",
    "\n",
    "# ここの値を変えると、結果が変わります#############\n",
    "# 各リンクの初期角度を定義\n",
    "link1_angle_deg = -90\n",
    "link2_angle_deg = 55\n",
    "link3_angle_deg = 35\n",
    "link4_angle_deg = 0\n",
    "link5_angle_deg = 0\n",
    "link6_angle_deg = 0\n",
    "feature_param = 10\n",
    "\n",
    "# 画像上の4つの特徴量の目標位置\n",
    "goal1_u = center_x - 50\n",
    "goal1_v = center_y - 50\n",
    "goal2_u = center_x + 50\n",
    "goal2_v = center_y - 50\n",
    "goal3_u = center_x + 50\n",
    "goal3_v = center_y + 50\n",
    "goal4_u = center_x - 50\n",
    "goal4_v = center_y + 50\n",
    "##############################################\n",
    "\n",
    "# deg -> radへ変換\n",
    "link1_angle_rad  = np.deg2rad(link1_angle_deg)\n",
    "link2_angle_rad  = np.deg2rad(link2_angle_deg)\n",
    "link3_angle_rad  = np.deg2rad(link3_angle_deg)\n",
    "link4_angle_rad  = np.deg2rad(link4_angle_deg)\n",
    "link5_angle_rad  = np.deg2rad(link5_angle_deg)\n",
    "link6_angle_rad  = np.deg2rad(link6_angle_deg)\n",
    "\n",
    "# 「4つの特徴量の目標位置」をリストにまとめる\n",
    "goal_feature_points = np.array([goal1_u, goal1_v, goal2_u, goal2_v, goal3_u, goal3_v, goal4_u, goal4_v])\n",
    "\n",
    "# ロボットの各関節のインデックス\n",
    "LINK1_JOINT_IDX = 0\n",
    "LINK2_JOINT_IDX = 1\n",
    "LINK3_JOINT_IDX = 2\n",
    "LINK4_JOINT_IDX = 3\n",
    "LINK5_JOINT_IDX = 4\n",
    "LINK6_JOINT_IDX = 5\n",
    "CAMERA_IDX = 6\n",
    "CAMERA_TARGET_IDX = 7\n",
    "\n",
    "# ロボットの各関節の角度を初期化\n",
    "pybullet.resetJointState(arm_id, LINK1_JOINT_IDX, link1_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK2_JOINT_IDX, link2_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK3_JOINT_IDX, link3_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK4_JOINT_IDX, link4_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK5_JOINT_IDX, link5_angle_rad )\n",
    "pybullet.resetJointState(arm_id, LINK6_JOINT_IDX, link6_angle_rad )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[ビジュアルサーボの実行](#toc0_)\n",
    "\n",
    "右側のスライダーの値を変更することで、ARマーカを貼り付けたボックスの位置・姿勢を変更することができます。\n",
    "\n",
    "なおデフォルトのロボットの姿勢、ボックスの位置の場合「x, roll, yaw」方向の変更は上手く調整しないと適切に動作しないことがあります。  \n",
    "そのため、最初は「y, z, pitch」方向の変更をお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# グローバル変数（おまけの章でも使用する変数をglobal変数として定義）\n",
    "current_feature_points = None # 特徴点の現在位置\n",
    "rgb_img = None # カメラから取得したRGB画像\n",
    "\n",
    "def VisualServo():\n",
    "    global current_feature_points\n",
    "    global rgb_img\n",
    "\n",
    "    while True:\n",
    "        # 「ARマーカが張り付けられたボックス」をスライダーに設定された位置、姿勢に設定\n",
    "        obj_x = pybullet.readUserDebugParameter(0)\n",
    "        obj_y = pybullet.readUserDebugParameter(1)\n",
    "        obj_z = pybullet.readUserDebugParameter(2)\n",
    "        obj_roll = pybullet.readUserDebugParameter(3)\n",
    "        obj_pitch = pybullet.readUserDebugParameter(4)\n",
    "        obj_yaw = pybullet.readUserDebugParameter(5)\n",
    "        pybullet.resetBasePositionAndOrientation(box_bid, [obj_x, obj_y, obj_z], pybullet.getQuaternionFromEuler([obj_roll, obj_pitch, obj_yaw]))\n",
    "\n",
    "        # 関節角度を取得\n",
    "        q1 = pybullet.getJointState(arm_id, LINK1_JOINT_IDX)[0]\n",
    "        q2 = pybullet.getJointState(arm_id, LINK2_JOINT_IDX)[0]\n",
    "        q3 = pybullet.getJointState(arm_id, LINK3_JOINT_IDX)[0]\n",
    "        q4 = pybullet.getJointState(arm_id, LINK4_JOINT_IDX)[0]\n",
    "        q5 = pybullet.getJointState(arm_id, LINK5_JOINT_IDX)[0]\n",
    "        q6 = pybullet.getJointState(arm_id, LINK6_JOINT_IDX)[0]\n",
    "\n",
    "        # ロボットアームの手先のカメラの姿勢に合わせて、カメラの上方向のベクトルを設定\n",
    "        camera_up_vector = np.array([0, -1, 0]) # デフォルトのカメラの上方向のベクトル\n",
    "        RW1 = Rz(theta=0)@Ry(theta=0)@Rx(theta=0) # ワールド座標系 -> link1座標系\n",
    "        R12 = Rz(theta=q1) # link1座標系 -> link2座標系\n",
    "        R23 = Ry(theta=q2) # link2座標系 -> link3座標系\n",
    "        R34 = Ry(theta=q3) # link3座標系 -> link4座標系\n",
    "        R45 = Rz(theta=q4) # link4座標系 -> link5座標系\n",
    "        R56 = Ry(theta=q5) # link5座標系 -> link6座標系\n",
    "        R6C = Rz(theta=q6-np.pi/2) # link6座標系 -> カメラ座標系（カメラを-90度回転すると、取得される画像が正しい向きになる）\n",
    "        RWC = RW1@R12@R23@R34@R45@R56@R6C # ワールド座標系 -> カメラ座標系\n",
    "        rotate_camera_up_vector = RWC@camera_up_vector # カメラ座標系におけるカメラの上方向のベクトル\n",
    "\n",
    "        # カメラ画像を取得\n",
    "        camera_link_pose = pybullet.getLinkState(arm_id, CAMERA_IDX)[0] # 手先のカメラリンクの位置\n",
    "        cameraTargetLinkPose = pybullet.getLinkState(arm_id, CAMERA_TARGET_IDX)[0] # カメラリンクの少しだけ前に設定した仮想的なリンクの位置\n",
    "        view_matrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]],cameraTargetPosition=[cameraTargetLinkPose[0], cameraTargetLinkPose[1], cameraTargetLinkPose[2]],cameraUpVector=rotate_camera_up_vector)\n",
    "        _, _, rgb_img, _, _ = pybullet.getCameraImage(\n",
    "            width=image_width,\n",
    "            height=image_height,\n",
    "            viewMatrix=view_matrix,\n",
    "            projectionMatrix=projection_matrix,\n",
    "            renderer=pybullet.ER_BULLET_HARDWARE_OPENGL\n",
    "        )\n",
    "\n",
    "        # 「画像座標系における4つの特徴量の位置（ピクセル位置）」、「カメラ→オブジェクトの距離[m]」を取得\n",
    "        current_feature_points, z = detect_ar_marker_corner_pos_and_depth(marker_size, aruco_dict, parameters, rgb_img, camera_matrix, dist_coeffs)\n",
    "\n",
    "        # 特徴量が検出できなかった場合は、次のループへ\n",
    "        if current_feature_points is None:\n",
    "            continue\n",
    "\n",
    "        # 画像ヤコビ行列\n",
    "        Ji = make_4features_image_jacobian_matrix(current_feature_points, z, f)\n",
    "        Ji_inv = np.linalg.pinv(Ji) # 画像ヤコビ行列の逆行列\n",
    "\n",
    "        # ワールド座標系から見た、各linkの座標系の位置を計算（基礎ヤコビ行列の計算に使用）\n",
    "        TW1 = Hz(theta=0)@Hy(theta=0)@Hx(theta=0)@Hp(x=ARM_ORIGIN_X_WORLD, y=ARM_ORIGIN_Y_WORLD, z=ARM_ORIGIN_Z_WORLD) # ワールド座標系 -> link1座標系\n",
    "        T12 = Hz(theta=q1)@Hp(x=0, y=0, z=LINK1_LENGTH) # リンク1座標系 -> リンク2座標系\n",
    "        T23 = Hy(theta=q2)@Hp(x=0, y=0, z=LINK2_LENGTH) # リンク2座標系 -> リンク3座標系\n",
    "        T34 = Hy(theta=q3)@Hp(x=0, y=0, z=LINK3_LENGTH) # リンク3座標系 -> リンク4座標系\n",
    "        T45 = Hz(theta=q4)@Hp(x=0, y=0, z=LINK4_LENGTH) # リンク4座標系 -> リンク5座標系\n",
    "        T56 = Hy(theta=q5)@Hp(x=0, y=0, z=LINK5_LENGTH) # リンク5座標系 -> リンク6座標系\n",
    "        T6C = Hz(theta=q6-np.pi/2)@Hp(x=CAMERA_X_OFFSET, y=0.0, z=LINK6_LENGTH-CAMERA_Z_SIZE/2) # link6座標系 -> カメラ座標系\n",
    "        origin_pos = np.array([0, 0, 0, 1]) # 原点位置\n",
    "        origin_link1_world = TW1@origin_pos                         # ワールド座標系から見た、リンク1座標系の原点位置\n",
    "        origin_link2_world = TW1@T12@origin_pos                     # ワールド座標系から見た、リンク2座標系の原点位置\n",
    "        origin_link3_world = TW1@T12@T23@origin_pos                 # ワールド座標系から見た、リンク3座標系の原点位置\n",
    "        origin_link4_world = TW1@T12@T23@T34@origin_pos             # ワールド座標系から見た、リンク4座標系の原点位置\n",
    "        origin_link5_world = TW1@T12@T23@T34@T45@origin_pos         # ワールド座標系から見た、リンク5座標系の原点位置\n",
    "        origin_link6_world = TW1@T12@T23@T34@T45@T56@origin_pos     # ワールド座標系から見た、リンク6座標系の原点位置\n",
    "        origin_linkC_world = TW1@T12@T23@T34@T45@T56@T6C@origin_pos # ワールド座標系から見た、カメラ座標系の原点位置\n",
    "\n",
    "        # ここから、ビジュアルサーボのメイン処理 ##########################################################\n",
    "        # 「現在の特徴量の位置」→「目標の特徴量の位置」方向のベクトル\n",
    "        feature_current_to_goal = goal_feature_points - current_feature_points\n",
    "\n",
    "        # 「現在の特徴量の位置」→「目標の特徴量の位置」方向に向かう微小変化量\n",
    "        delta_feature_points = feature_param * feature_current_to_goal\n",
    "\n",
    "        # 「画像ヤコビ行列の逆行列Ji_inv」を用いて、「特徴点の微小変化量delta_feature_points」から、「カメラの微小変化量（=Δx, Δy, Δz, Δroll, Δpitch, Δyaw）camera_delta_p」を求める\n",
    "        # （カメラをどのように動かせば、特徴点が目標の位置に移動するか）\n",
    "        delta_p_camera = Ji_inv @ delta_feature_points\n",
    "\n",
    "        # Rを対角に持った対角行列\n",
    "        size = 6\n",
    "        rotate_diagonal_matrix = np.zeros((size, size))\n",
    "        for i in range(0, size, 3):\n",
    "            rotate_diagonal_matrix[i:i+3, i:i+3] = RWC\n",
    "\n",
    "        # 「回転行列rotate_diagonal_matrix」を用いて「カメラ座標系を基準としたカメラの微小変化量delta_p_camera」から「ワールド座標系を基準としたカメラの微小変化量delta_p_world」に変換する\n",
    "        delta_p_world = rotate_diagonal_matrix @ delta_p_camera\n",
    "\n",
    "        # 基礎ヤコビ行列\n",
    "        Jv = make_6d_jacobian_matrix(R=[R12, R23, R34, R45, R56, R6C],\n",
    "                                     o1=origin_link1_world[0:3], \n",
    "                                     o2=origin_link2_world[0:3], \n",
    "                                     o3=origin_link3_world[0:3], \n",
    "                                     o4=origin_link4_world[0:3], \n",
    "                                     o5=origin_link5_world[0:3], \n",
    "                                     o6=origin_link6_world[0:3], \n",
    "                                     oc=origin_linkC_world[0:3])\n",
    "        Jv_inv = np.linalg.pinv(Jv) # 基礎ヤコビ行列の逆行列\n",
    "\n",
    "        # 「（基礎）ヤコビ行列の逆行列Jv_inv」を用いて、「ワールド座標系を基準としたカメラの微小変化量delta_p_world」から、「各関節の角速度delta_q」を求める\n",
    "        # （ロボットアームの各関節をどのように動かせば、カメラが目標の位置、姿勢に移動するか）\n",
    "        delta_q = Jv_inv @ delta_p_world\n",
    "\n",
    "        # 求めた角速度を各関節にセット\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK1_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[0])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK2_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[1])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK3_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[2])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK4_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[3])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK5_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[4])\n",
    "        pybullet.setJointMotorControl2(arm_id, LINK6_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=delta_q[5])\n",
    "\n",
    "        # 1時刻分だけ進める\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(time_step)\n",
    "\n",
    "# スレッドを立ち上げる\n",
    "thread = threading.Thread(target=VisualServo)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[（おまけ）特徴点位置の描画](#toc0_)\n",
    "\n",
    "下コードを実行すると、「ゴールの特徴点位置」と「現在の特徴点位置」が描画されます。\n",
    "\n",
    "具体的に、特徴点がどのように移動しているのかを確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "plt.ion()  # インタラクティブモードを有効にする\n",
    "fig, ax = plt.subplots()\n",
    "goal_plot, = ax.plot([], [], 'ro', label=\"Goal Points\")\n",
    "current_plot, = ax.plot([], [], 'bo', label=\"Current Points\")\n",
    "ax.set_xlim(0, image_width)\n",
    "ax.set_ylim(0, image_height)\n",
    "ax.invert_yaxis()  # 画像座標系と一致させるためにy軸を反転\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# メインループで、カメラから取得した画像を描画\n",
    "initDraw = True\n",
    "while True:\n",
    "    # rgbImgを描画し、goal_feature_pointsとcurrent_feature_pointsを描画\n",
    "    try:\n",
    "        if goal_feature_points is not None and current_feature_points is not None:\n",
    "            if initDraw:\n",
    "                img_plot = ax.imshow(rgb_img)\n",
    "                initDraw = False\n",
    "            else:\n",
    "                img_plot.set_data(rgb_img)\n",
    "            goal_plot.set_xdata([goal1_u, goal2_u, goal3_u, goal4_u])\n",
    "            goal_plot.set_ydata([goal1_v, goal2_v, goal3_v, goal4_v])\n",
    "            current_plot.set_xdata([current_feature_points[0], current_feature_points[2], current_feature_points[4], current_feature_points[6]])\n",
    "            current_plot.set_ydata([current_feature_points[1], current_feature_points[3], current_feature_points[5], current_feature_points[7]])\n",
    "            plt.pause(0.001)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
