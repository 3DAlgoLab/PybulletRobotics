{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [移動ロボットによる強化学習を用いたライントレース](#toc1_)    \n",
    "- [参考サイトなど](#toc2_)    \n",
    "- [pybulletの起動](#toc3_)    \n",
    "- [初期設定](#toc4_)    \n",
    "- [Q学習クラスの定義](#toc5_)    \n",
    "  - [Q学習とは](#toc5_1_)    \n",
    "  - [Q値と報酬](#toc5_2_)    \n",
    "  - [Q値の更新](#toc5_3_)    \n",
    "  - [Qテーブル](#toc5_4_)    \n",
    "- [Qテーブルの初期化](#toc6_)    \n",
    "- [Q学習のパラメータ設定](#toc7_)    \n",
    "- [Q学習の実行](#toc8_)    \n",
    "- [おまけ：ライントレース用の画像生成](#toc9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Line Tracing Using Reinforcement Learning with a Mobile Robot](#toc0_)\n",
    "\n",
    "In this notebook, we will perform line tracing using a two-wheeled mobile robot with reinforcement learning.\n",
    "\n",
    "(For a manual summarizing the functions available in pybullet, please refer to [here](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Starting pybullet](#toc0_)\n",
    "\n",
    "Start pybullet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import pybullet\n",
    "import pybullet_data\n",
    "physics_client = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Initial Setup](#toc0_)\n",
    "\n",
    "Perform initial setup such as generating the floor, creating box objects, generating the robot, and setting the camera position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # Reset the simulation space\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # Add path to necessary data for pybullet\n",
    "pybullet.setGravity(0.0, 0.0, -9.8) # Set gravity to Earth's gravity\n",
    "time_step = 1./240.\n",
    "pybullet.setTimeStep(time_step)\n",
    "\n",
    "# Load the floor\n",
    "plane_id = pybullet.loadURDF(\"plane.urdf\", basePosition=[-5.0, -5.0, 0.0], globalScaling=5.0)\n",
    "tex_uid = pybullet.loadTexture(\"../texture/line_trace_ground.png\")\n",
    "pybullet.changeVisualShape(plane_id, -1, textureUniqueId=tex_uid)\n",
    "\n",
    "# Load the robot\n",
    "car_start_pos = [0.0, 0.0, 0.1]  # Set initial position (x, y, z)\n",
    "car_start_orientation = pybullet.getQuaternionFromEuler([0, 0, 0])  # Set initial orientation (roll, pitch, yaw)\n",
    "car_id = pybullet.loadURDF(\"../urdf/simple_two_wheel_car.urdf\", car_start_pos, car_start_orientation)\n",
    "\n",
    "# Set camera position for GUI mode\n",
    "camera_distance = 6.0\n",
    "camera_yaw = 180.0 # deg\n",
    "camera_pitch = -90.1 # deg\n",
    "camera_target_position = [0.0, 1.0, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation matrices to calculate cameraUpVector according to the movement of the mobile robot\n",
    "def Rx(theta):\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Definition of Q-Learning Class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[What is Q-Learning?](#toc0_)\n",
    "\n",
    "Q-Learning is a type of reinforcement learning where the robot learns to select actions $a$ that maximize the Q-value $Q(s, a)$ when it takes a state $s$.\n",
    "\n",
    "In this case, as shown in the figure below:\n",
    "\n",
    "- The state $s_t$ is defined as the \"color on the left\" and \"color on the right\" obtained from the mobile robot's camera.\n",
    "- The action $a_t$ is defined as the robot's driving direction (straight, turn right, turn left).\n",
    "\n",
    "We will perform Q-Learning based on these definitions.\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/sim_environment.png\" width=\"80%\">\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/sim_environment.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Q-Values and Rewards](#toc0_)\n",
    "\n",
    "In Q-learning, the following two concepts are important:\n",
    "\n",
    "- \"Q-value $Q(s, a)$\"\n",
    "- \"Reward $r$\"\n",
    "\n",
    "These are similar but different concepts.\n",
    "\n",
    "\"Reward $r$\" represents the reward obtained after taking action $a$ in state $s$.\n",
    "In this case,\n",
    "- After taking action $a_t$ in state $s_t$, if the \"left color is black\" and \"right color is black\", a reward $r_{t+1}=+1$ is given,\n",
    "- If the \"left color is white\" and \"right color is white\", a reward $r_{t+1}=-1$ is given,\n",
    "- In other cases, no reward is given\n",
    "\n",
    "This is set up as described above (also called immediate reward).\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/reward.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/reward.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the \"Q-value $Q(s, a)$\" represents the value of taking action $a$ in state $s$, **considering future rewards**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "tips: Specific Example of the Difference Between Reward and Q-Value\n",
    "\n",
    "Let's compare rewards and Q-values with a specific example.\n",
    "Since the current example is a bit difficult to understand, let's use the example of \"Go\".\n",
    "- In the case of Go, the reward is \"win or lose\". A reward of $r=+1$ is given for a win, and a reward of $r=-1$ is given for a loss.\n",
    "- On the other hand, the Q-value represents \"the probability of ultimately winning if the next move $a$ is made in the current situation $s$.\"\n",
    "\n",
    "In 2015, DeepMind's \"AlphaGo\" succeeded in defeating the world champion by learning the strength of Go using Q-values.\n",
    "This was the result of learning the strategy of \"taking actions that maximize future rewards (victory), even if they seem disadvantageous at the moment\" with high accuracy (surpassing human professional players).\n",
    "\n",
    "Note that AlphaGo uses a method called Deep Q-Network (DQN), which combines Q-learning with neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Updating Q-Values](#toc0_)\n",
    "\n",
    "In Q-learning, learning is performed by repeatedly updating the Q-values using the following equation:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "Here, the meanings of each variable are as follows:\n",
    "- $\\alpha$: Learning rate\n",
    "- $r_{t+1}$: Reward obtained immediately after taking action $a_t$ in state $s_t$\n",
    "- $\\gamma$: Discount rate (how much future rewards are considered)\n",
    "- $\\max_{a} Q(s_{t+1}, a)$: The highest Q-value among the possible actions $a$ in state $s_{t+1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "The term $r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)$ is called the \"TD error\".  \n",
    "The TD error can be defined as $\\delta = r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)$, as shown below (this definition is also used in the code).\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/td_error.png\" width=\"60%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In particular, the term $r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a)$ is called the \"TD target\", and the Q-value is updated to approach this TD target.\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/td_target.png\" width=\"60%\">\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "---\n",
    "\n",
    "tips:\n",
    "\n",
    "他の強化学習手法に、SARSA（State-Action-Reward-State-Action）という手法があります。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_learn_and_sarsa.png\" width=\"60%\">\n",
    "\n",
    "Q学習が「最もQ値が高い行動を選択する」のに対して、SARSAは「実際に取った行動に対するQ値を更新する」という違いがあります。\n",
    "\n",
    "---\n",
    "\n",
    "--- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Q-Table](#toc0_)\n",
    "\n",
    "So far, we have explained Q-values, but since Q-values exist for each state $s$ and action $a$ pair, each Q-value is managed in a table called a \"Q-Table\".\n",
    " \n",
    "| states \\ actions | $a_0$ | $a_1$ |...| $a_n$ |\n",
    "|:-----------------:|:-----:|:-----:|:-:|:-----:|\n",
    "| $s_0$             | $Q(s_0, a_0)$ | $Q(s_0, a_1)$ |...| $Q(s_0, a_n)$ |\n",
    "| $s_1$             | $Q(s_1, a_0)$ | $Q(s_1, a_1)$ |...| $Q(s_1, a_n)$ |\n",
    "| ...               | ... | ... |...| ... |\n",
    "| $s_m$             | $Q(s_m, a_0)$ | $Q(s_m, a_1)$ |...| $Q(s_m, a_n)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table for this line tracing problem is shown below.\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_table.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this line tracing problem,\n",
    "- Go straight when \"black-black\"\n",
    "- Turn right when \"white-black\"\n",
    "- Turn left when \"black-white\"\n",
    "\n",
    "Therefore, the Q-table after learning should look like the figure below (the parts labeled \"High\" should have high Q-values after learning).\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_table_after_learning.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "*Note: For educational purposes, we have set up a simple problem where the Q-table after learning can be predicted.  \n",
    "In reality, it is difficult to predict the Q-table for complex problems, so it is important to set appropriate rewards and other parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class Actions(IntEnum):\n",
    "    \"\"\"\n",
    "    Definition of actions (columns of Q-table)\n",
    "    \"\"\"\n",
    "    STRAIGHT = 0\n",
    "    TURN_LEFT = 1\n",
    "    TURN_RIGHT = 2\n",
    "\n",
    "class States(IntEnum):\n",
    "    \"\"\"\n",
    "    Definition of states (rows of Q-table)\n",
    "    \"\"\"\n",
    "    BLACK_BLACK = 0\n",
    "    WHITE_BLACK = 1\n",
    "    BLACK_WHITE = 2\n",
    "    WHITE_WHITE = 3\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, q_table, LEARNING_RATE=0.1, DISCOUNT_RATE=0.9, EPSILON=0.1):\n",
    "        \"\"\"\n",
    "        Initialization of Q-learning\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_table : list\n",
    "            Q-table\n",
    "        LEARNING_RATE : float\n",
    "            Learning rate\n",
    "        DISCOUNT_RATE : float\n",
    "            Discount rate\n",
    "        EPSILON : float\n",
    "            Probability of selecting a random action\n",
    "        \"\"\" \n",
    "        self.q_table = q_table\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.DISCOUNT_RATE = DISCOUNT_RATE\n",
    "        self.EPSILON = EPSILON   \n",
    "\n",
    "    def get_state(self, car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix):\n",
    "        \"\"\"\n",
    "        Get state\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        car_id : int\n",
    "            ID of the robot\n",
    "        CAMERA_IDX : int\n",
    "            Index of the camera link\n",
    "        CAMERA_TARGET_IDX : int\n",
    "            Index of the virtual link for the target point\n",
    "        projection_matrix : list\n",
    "            Projection matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : int\n",
    "            State s_t\n",
    "        \"\"\"\n",
    "        # Default direction of camera_up_vector\n",
    "        camera_up_vector = np.array([0, -1, 0])\n",
    "        \n",
    "        # Get the position of the camera link\n",
    "        camera_link_pose = pybullet.getLinkState(car_id, CAMERA_IDX)[0]\n",
    "\n",
    "        # Get the position of the virtual link for the target point\n",
    "        camera_target_link_pose = pybullet.getLinkState(car_id, CAMERA_TARGET_IDX)[0] \n",
    "\n",
    "        # Rotate cameraUpVector to match the posture of the mobile robot\n",
    "        mobile_robot_roll, mobile_robot_pitch, mobile_robot_yaw = pybullet.getEulerFromQuaternion(pybullet.getLinkState(car_id, CAMERA_IDX)[1])\n",
    "        R = Rz(np.deg2rad(90.0) + mobile_robot_yaw) @ Ry(mobile_robot_pitch) @ Rx(mobile_robot_roll)\n",
    "        rotate_camera_up_vector = R @ camera_up_vector\n",
    "\n",
    "        # Get the viewMatrix in the direction from the camera link to the virtual link for the target point\n",
    "        view_matrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]], cameraTargetPosition=[camera_target_link_pose[0], camera_target_link_pose[1], camera_target_link_pose[2]], cameraUpVector=[rotate_camera_up_vector[0], rotate_camera_up_vector[1], rotate_camera_up_vector[2]])\n",
    "        \n",
    "        # Get the image of the line\n",
    "        width, height, rgb_img, _, _ = pybullet.getCameraImage(600, 300, view_matrix, projection_matrix, renderer=pybullet.ER_BULLET_HARDWARE_OPENGL, shadow=0, flags=pybullet.ER_NO_SEGMENTATION_MASK)\n",
    "\n",
    "        # Binarize the image\n",
    "        img = np.reshape(rgb_img, (height, width, 4))  # Convert the obtained image to a 4-channel numpy array\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)  # Binarize\n",
    "\n",
    "        # Get the color of the pixel slightly to the left of the center of the image\n",
    "        color_left = binary_img[height // 2, width // 2 - 150]\n",
    "\n",
    "        # Get the color of the pixel slightly to the right of the center of the image\n",
    "        color_right = binary_img[height // 2, width // 2 + 150]\n",
    "\n",
    "        # Get the state\n",
    "        if color_left == 0 and color_right == 0:\n",
    "            state = States.BLACK_BLACK\n",
    "        elif color_left == 255 and color_right == 0:\n",
    "            state = States.WHITE_BLACK\n",
    "        elif color_left == 0 and color_right == 255:\n",
    "            state = States.BLACK_WHITE\n",
    "        elif color_left == 255 and color_right == 255:\n",
    "            state = States.WHITE_WHITE\n",
    "        return state\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            State s_t\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Action a_t\n",
    "        \"\"\"\n",
    "\n",
    "        # Action selection using ε-greedy method\n",
    "        if np.random.uniform(0, 1) < self.EPSILON:\n",
    "            # Select a random action with probability epsilon\n",
    "            action = np.random.choice(list(Actions))\n",
    "        else:\n",
    "            # Select the action with the maximum Q value with probability 1-epsilon\n",
    "            action = Actions(np.argmax(self.q_table[state][:]))\n",
    "        return action\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        Get reward\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            State s_t\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Reward r_t\n",
    "        \"\"\"\n",
    "        if state == States.BLACK_BLACK:\n",
    "            reward = 1\n",
    "        elif state == States.BLACK_WHITE:\n",
    "            reward = 0\n",
    "        elif state == States.WHITE_BLACK:\n",
    "            reward = 0\n",
    "        elif state == States.WHITE_WHITE:\n",
    "            reward = -1\n",
    "        return reward\n",
    "    \n",
    "    def update_q_table(self, state, action, state_next, reward):\n",
    "        \"\"\"\n",
    "        Update Q-table\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            State s_t at time t\n",
    "        action : int\n",
    "            Action a_t at time t\n",
    "        state_next : int\n",
    "            State s_{t+1} at time t+1\n",
    "        reward : float\n",
    "            Reward r_{t+1} at time t+1\n",
    "            ※ The reward obtained as a result of transitioning from \"state s{t} at time t\" to \"state s_{t+1} at time t+1\" by taking \"action a_t at time t\", so the subscript is t+1\n",
    "        \"\"\"\n",
    "        # Find the maximum Q value among the possible actions \"a\" in \"state s_{t+1} at time t+1\"\n",
    "        max_q_value = max(self.q_table[state_next][:])\n",
    "\n",
    "        # Calculate TD error\n",
    "        td_error = reward + self.DISCOUNT_RATE * max_q_value - self.q_table[state][action]\n",
    "                                                            \n",
    "        # Update Q-table\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.LEARNING_RATE * td_error\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Initialization of Q-Table](#toc0_)\n",
    "\n",
    "Initialize the Q-table.\n",
    "\n",
    "Even if the learning process is interrupted, you can resume learning from where you left off by saving the values of this Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((len(States), len(Actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Setting Parameters for Q-Learning](#toc0_)\n",
    "\n",
    "Set the parameters for Q-learning.\n",
    "\n",
    "By changing the following parameters, the speed and performance of learning will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_MAX = 1000 # Maximum number of episodes\n",
    "STEP_MAX = 4000 # Maximum number of steps per episode\n",
    "LEARNING_RATE = 0.01 # Learning rate\n",
    "DISCOUNT_RATE = 0.9 # Discount rate\n",
    "EPSILON = 0.1 # Epsilon for ε-greedy method (probability of selecting a random action)\n",
    "LEARNING_COMPLETE_EPISODE_NUM = 4 # Number of episodes to determine learning completion (In this case, learning is considered complete if the mobile robot can complete a lap without going off the line for \"LEARNING_COMPLETE_EPISODE_NUM\" consecutive times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Execution of Q-Learning](#toc0_)\n",
    "\n",
    "Q-Learning is executed in the following steps:\n",
    "\n",
    "1. Obtain the initial state $s_t$\n",
    "2. Select and execute the action $a_t$ with the highest Q-value in state $s_t$ from the Q-table\n",
    "    - However, a random action is selected with a certain probability (ε-greedy method)\n",
    "3. Obtain the next state $s_{t+1}$\n",
    "4. Obtain the reward $r_{t+1}$\n",
    "5. Update the Q-value $Q(s_t, a_t)$ and reflect it in the Q-table\n",
    "6. Update the state $s_t$ to $s_{t+1}$\n",
    "7. Repeat steps 2 to 6 until learning is complete or the maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m240.\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 次の状態を取得\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m state_next \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcar_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCAMERA_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCAMERA_TARGET_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 報酬を取得\u001b[39;00m\n\u001b[1;32m    106\u001b[0m reward \u001b[38;5;241m=\u001b[39m q_learning\u001b[38;5;241m.\u001b[39mget_reward(state_next)\n",
      "Cell \u001b[0;32mIn[4], line 82\u001b[0m, in \u001b[0;36mQLearning.get_state\u001b[0;34m(self, car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\u001b[0m\n\u001b[1;32m     79\u001b[0m view_matrix \u001b[38;5;241m=\u001b[39m pybullet\u001b[38;5;241m.\u001b[39mcomputeViewMatrix(cameraEyePosition\u001b[38;5;241m=\u001b[39m[camera_link_pose[\u001b[38;5;241m0\u001b[39m], camera_link_pose[\u001b[38;5;241m1\u001b[39m], camera_link_pose[\u001b[38;5;241m2\u001b[39m]],cameraTargetPosition\u001b[38;5;241m=\u001b[39m[camera_target_link_pose[\u001b[38;5;241m0\u001b[39m], camera_target_link_pose[\u001b[38;5;241m1\u001b[39m], camera_target_link_pose[\u001b[38;5;241m2\u001b[39m]],cameraUpVector\u001b[38;5;241m=\u001b[39m[rotate_camera_up_vector[\u001b[38;5;241m0\u001b[39m], rotate_camera_up_vector[\u001b[38;5;241m1\u001b[39m], rotate_camera_up_vector[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# ラインの画像を取得\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m width, height, rgb_img, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetCameraImage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mER_BULLET_HARDWARE_OPENGL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshadow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mER_NO_SEGMENTATION_MASK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 画像の2値化\u001b[39;00m\n\u001b[1;32m     85\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(rgb_img, (height, width, \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# 取得した画像を4チャンネルのnumpy配列に変換\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set the robot to the initial position\n",
    "car_start_pos = [3.0, 0, 0.1]\n",
    "car_start_orientation = pybullet.getQuaternionFromEuler([0.0, 0.0, -math.pi/2])\n",
    "pybullet.resetBasePositionAndOrientation(car_id, car_start_pos, car_start_orientation)\n",
    "\n",
    "# Bottom camera settings\n",
    "projection_matrix = pybullet.computeProjectionMatrixFOV(fov=140.0, aspect=1.0, nearVal=0.04, farVal=100)\n",
    "\n",
    "# Link indices\n",
    "CAMERA_IDX = 8\n",
    "CAMERA_TARGET_IDX = 9\n",
    "\n",
    "# Joint indices\n",
    "RIGHT_WHEEL_JOINT_IDX = 0\n",
    "LEFT_WHEEL_JOINT_IDX = 1\n",
    "\n",
    "# Speed when moving straight\n",
    "BASE_SPEED = 30\n",
    "\n",
    "# Initialize Q-learning\n",
    "q_learning = QLearning(q_table, LEARNING_RATE, DISCOUNT_RATE, EPSILON)\n",
    "episode_complete_cnt = 0 # Number of times the mobile robot has completed a lap without going off the line consecutively\n",
    "\n",
    "# Start learning\n",
    "for episode in range(EPISODE_MAX):\n",
    "\n",
    "    # End if learning is complete\n",
    "    if episode_complete_cnt == LEARNING_COMPLETE_EPISODE_NUM:\n",
    "        pybullet.removeAllUserDebugItems()\n",
    "        pybullet.addUserDebugText(f\"Learning is complete!!\", [-2.3, 6.0, 0.1], textSize=2, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"Episode: {episode}\", [-0.7, 5.5, 0.1], textSize=1.5, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"Q table\", [-0.5, 5.0, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"| STATE \\ ACTION | STRAIGHT | TURN LEFT | TURN RIGHT |\", [-3.9, 4.6, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"----------------------------------------------------------------------------\", [-3.9, 4.4, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   BLACK-BLACK   |    {q_table[0][0]:.2f}       |     {q_table[0][1]:.2f}        |      {q_table[0][2]:.2f}        |\", [-3.9, 4.2, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   WHITE-BLACK   |    {q_table[1][0]:.2f}       |     {q_table[1][1]:.2f}        |      {q_table[1][2]:.2f}        |\", [-3.9, 3.8, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   BLACK-WHITE   |    {q_table[2][0]:.2f}       |     {q_table[2][1]:.2f}        |      {q_table[2][2]:.2f}        |\", [-3.9, 3.4, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   WHITE-WHITE   |    {q_table[3][0]:.2f}       |     {q_table[3][1]:.2f}        |      {q_table[3][2]:.2f}        |\", [-3.9, 3.0, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        break\n",
    "\n",
    "    # Set the mobile robot to the initial position and speed 0\n",
    "    # Change the direction of travel for each episode\n",
    "    if episode % 2 == 0:\n",
    "        car_start_pos = [3.0, 0, 0.1]\n",
    "        car_yaw = -math.pi/2\n",
    "    else:\n",
    "        car_yaw = math.pi/2\n",
    "        car_start_pos = [3.0, 0, 0.1]\n",
    "    car_start_orientation = pybullet.getQuaternionFromEuler([0.0, 0.0, car_yaw])\n",
    "    pybullet.resetBasePositionAndOrientation(car_id, car_start_pos, car_start_orientation)\n",
    "    pybullet.setJointMotorControl2(car_id, RIGHT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=0)\n",
    "    pybullet.setJointMotorControl2(car_id, LEFT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=0)\n",
    "\n",
    "    # Display Q table for each episode\n",
    "    # (In reality, the Q table is updated every step, but addUserDebugText is heavy, so it is displayed for each episode)\n",
    "    pybullet.removeAllUserDebugItems()\n",
    "    pybullet.addUserDebugText(f\"Episode: {episode+1}\", [-0.7, 5.5, 0.1], textSize=1.5, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"Q table\", [-0.5, 5.0, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"| STATE \\ ACTION | STRAIGHT | TURN LEFT | TURN RIGHT |\", [-3.9, 4.6, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"----------------------------------------------------------------------------\", [-3.9, 4.4, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   BLACK-BLACK   |    {q_table[0][0]:.2f}       |     {q_table[0][1]:.2f}        |      {q_table[0][2]:.2f}        |\", [-3.9, 4.2, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   WHITE-BLACK   |    {q_table[1][0]:.2f}       |     {q_table[1][1]:.2f}        |      {q_table[1][2]:.2f}        |\", [-3.9, 3.8, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   BLACK-WHITE   |    {q_table[2][0]:.2f}       |     {q_table[2][1]:.2f}        |      {q_table[2][2]:.2f}        |\", [-3.9, 3.4, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   WHITE-WHITE   |    {q_table[3][0]:.2f}       |     {q_table[3][1]:.2f}        |      {q_table[3][2]:.2f}        |\", [-3.9, 3.0, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "\n",
    "    # 1. Get the initial state\n",
    "    state = q_learning.get_state(car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\n",
    "\n",
    "    # Take a maximum of STEP_MAX actions in one episode\n",
    "    for step in range(STEP_MAX):\n",
    "\n",
    "        # 2. Select and execute an action from the Q table\n",
    "        action = q_learning.get_action(state)\n",
    "        if action == Actions.STRAIGHT:\n",
    "            left_speed = BASE_SPEED\n",
    "            right_speed = BASE_SPEED\n",
    "        elif action == Actions.TURN_LEFT:\n",
    "            left_speed = BASE_SPEED - 10\n",
    "            right_speed = BASE_SPEED\n",
    "        elif action == Actions.TURN_RIGHT:\n",
    "            left_speed = BASE_SPEED\n",
    "            right_speed = BASE_SPEED - 10\n",
    "        pybullet.setJointMotorControl2(car_id, RIGHT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=right_speed)\n",
    "        pybullet.setJointMotorControl2(car_id, LEFT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=left_speed)\n",
    "\n",
    "        # Advance the simulation by one time step\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(1./240.)\n",
    "\n",
    "        # 3. Get the next state\n",
    "        state_next = q_learning.get_state(car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\n",
    "\n",
    "        # 4. Get the reward\n",
    "        reward = q_learning.get_reward(state_next)\n",
    "\n",
    "        # 5. Update the Q table\n",
    "        q_learning.update_q_table(state, action, state_next, reward)\n",
    "\n",
    "        # 6. Update the state\n",
    "        state = state_next\n",
    "\n",
    "        # End the episode if the mobile robot goes off the line\n",
    "        if state_next == States.WHITE_WHITE:\n",
    "            episode_complete_cnt = 0\n",
    "            break\n",
    "\n",
    "        # End the episode if the mobile robot completes a lap (returns near the starting point)\n",
    "        current_car_pos = pybullet.getBasePositionAndOrientation(car_id)[0]\n",
    "        current_to_start_distance = (car_start_pos[0] - current_car_pos[0])**2 + (car_start_pos[1] - current_car_pos[1])**2\n",
    "        if current_to_start_distance < 0.1**2 and step > 1000:\n",
    "            episode_complete_cnt += 1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Bonus: Generating an Image for Line Tracing](#toc0_)\n",
    "\n",
    "The following code generates an image with a circle of the specified radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X connection to :0 broken (explicit kill or server shutdown).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "dpi = 100\n",
    "\n",
    "plane_width = 10 \n",
    "plane_height = 10\n",
    "line_radius = 3\n",
    "\n",
    "image_width = (plane_width * dpi)\n",
    "image_height = (plane_height * dpi)\n",
    "\n",
    "image = Image.new('RGB', (image_width, image_height), (255, 255, 255))\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "def gazebo_to_image_coords(x, y):\n",
    "    x_image = int(((x + plane_width/2) / plane_width) * image_width)\n",
    "    y_image = int((1 - (y + plane_height/2) / plane_height) * image_height)\n",
    "    return x_image, y_image\n",
    "\n",
    "center_x_gazebo, center_y_gazebo = 0, 0\n",
    "\n",
    "center_x_image, center_y_image = gazebo_to_image_coords(center_x_gazebo, center_y_gazebo)\n",
    "radius_image = int(line_radius * image_width / plane_width)\n",
    "\n",
    "draw.ellipse((center_x_image - radius_image, center_y_image - radius_image, center_x_image + radius_image, center_y_image + radius_image), outline=\"black\", width=15)\n",
    "\n",
    "image.save('../texture/line_trace_ground.png', dpi=(dpi, dpi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
