{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [移動ロボットによる強化学習を用いたライントレース](#toc1_)    \n",
    "- [参考サイトなど](#toc2_)    \n",
    "- [pybulletの起動](#toc3_)    \n",
    "- [初期設定](#toc4_)    \n",
    "- [Q学習クラスの定義](#toc5_)    \n",
    "  - [Q学習とは](#toc5_1_)    \n",
    "  - [Q値と報酬](#toc5_2_)    \n",
    "  - [Q値の更新](#toc5_3_)    \n",
    "  - [Qテーブル](#toc5_4_)    \n",
    "- [Qテーブルの初期化](#toc6_)    \n",
    "- [Q学習のパラメータ設定](#toc7_)    \n",
    "- [Q学習の実行](#toc8_)    \n",
    "- [おまけ：ライントレース用の画像生成](#toc9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[移動ロボットによる強化学習を用いたライントレース](#toc0_)\n",
    "\n",
    "本notebookでは2輪の移動ロボットを用いて、強化学習を用いたライントレースを行います。\n",
    "\n",
    "（pybulletで使用可能な関数がまとめられたマニュアルについては[こちら](https://github.com/bulletphysics/bullet3/blob/master/docs/pybullet_quickstartguide.pdf)を参照してください。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[参考サイトなど](#toc0_)\n",
    "- [【強化学習】TD学習](https://www.tcom242242.net/entry/ai-2/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/%E3%80%90%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%80%91td%E5%AD%A6%E7%BF%92%EF%BC%88td0/)\n",
    "- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)\n",
    "- [強化学習手法の一つ「Q学習」をなるべくわかりやすく解説してみた](https://www.cresco.co.jp/blog/entry/entry-1676566754114828351.html)\n",
    "- [Q学習の式を理解する](https://note.com/pumonmon/n/n04f9139ad826#0V6ch)\n",
    "- [【入門】Q学習の解説とpythonでの実装 〜シンプルな迷路問題を例に〜](https://www.tcom242242.net/entry/ai-2/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/%E3%80%90%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%80%81%E5%85%A5%E9%96%80%E3%80%91q%E5%AD%A6%E7%BF%92_%E8%BF%B7%E8%B7%AF%E3%82%92%E4%BE%8B%E3%81%AB/#google_vignette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[pybulletの起動](#toc0_)\n",
    "\n",
    "pybulletを起動します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa\n",
      "GL_RENDERER=llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Mesa\n",
      "Renderer = llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import pybullet\n",
    "import pybullet_data\n",
    "physics_client = pybullet.connect(pybullet.GUI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[初期設定](#toc0_)\n",
    "\n",
    "床の生成、ボックスオブジェクトの生成、ロボットの生成、カメラ位置の設定などの初期設定を行います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = Mesa\n",
      "ven = Mesa\n"
     ]
    }
   ],
   "source": [
    "pybullet.resetSimulation() # シミュレーション空間をリセット\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath()) # pybulletに必要なデータへのパスを追加\n",
    "pybullet.setGravity(0.0, 0.0, -9.8) # 地球上における重力に設定\n",
    "time_step = 1./240.\n",
    "pybullet.setTimeStep(time_step)\n",
    "\n",
    "#床の読み込み\n",
    "plane_id = pybullet.loadURDF(\"plane.urdf\",  basePosition=[-5.0, -5.0, 0.0], globalScaling=5.0)\n",
    "tex_uid = pybullet.loadTexture(\"../texture/line_trace_ground.png\")\n",
    "pybullet.changeVisualShape(plane_id, -1, textureUniqueId=tex_uid)\n",
    "\n",
    "# ロボットの読み込み\n",
    "car_start_pos = [0.0, 0.0, 0.1]  # 初期位置(x,y,z)を設定\n",
    "car_start_orientation = pybullet.getQuaternionFromEuler([0, 0, 0])  # 初期姿勢(roll, pitch, yaw)を設定\n",
    "car_id = pybullet.loadURDF(\"../urdf/simple_two_wheel_car.urdf\",car_start_pos, car_start_orientation)\n",
    "\n",
    "# GUIモードの際のカメラの位置などを設定\n",
    "camera_distance = 6.0\n",
    "camera_yaw = 180.0 # deg\n",
    "camera_pitch = -90.1 # deg\n",
    "# cameraTargetPosition = [4.0, 0.0, 0.0]\n",
    "# camera_target_position = [2.5, 0.0, 0.0]\n",
    "camera_target_position = [0.0, 1.0, 0.0]\n",
    "pybullet.resetDebugVisualizerCamera(camera_distance, camera_yaw, camera_pitch, camera_target_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移動ロボットの動作に合わせて、cameraUpVectorを計算するために使用する回転行列を定義\n",
    "def Rx(theta):\n",
    "    return np.array([[1, 0, 0],\n",
    "                     [0, np.cos(theta), -np.sin(theta)],\n",
    "                     [0, np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "def Ry(theta):\n",
    "    return np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "                     [0, 1, 0],\n",
    "                     [-np.sin(theta), 0, np.cos(theta)]])\n",
    "\n",
    "def Rz(theta):\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Q学習クラスの定義](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Q学習とは](#toc0_)\n",
    "\n",
    "Q学習とは、強化学習の一種で\n",
    "\n",
    "ロボットが「状態$s$」を取った時に、「Q値 $Q(s, a)$」を最大化するような「行動$a$」を選択するように学習する手法です。\n",
    "\n",
    "今回は、下図のように\n",
    "\n",
    "- 移動ロボットのカメラから得られた「左側の色」と「右側の色」を状態$s_t$\n",
    "- ロボットの走行方向（直進、右折、左折）を行動$a_t$\n",
    "\n",
    "として、Q学習を行います。\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/sim_environment.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Q値と報酬](#toc0_)\n",
    "\n",
    "Q学習において、以下の2つの概念が重要です。\n",
    "\n",
    "- 「Q値 $Q(s, a)$」\n",
    "- 「報酬$r$」\n",
    "\n",
    "これらは、似ているようで異なる概念です。\n",
    "\n",
    "「報酬$r$」は、状態$s$において行動$a$を取った後に得られる報酬を表します。\n",
    "今回の場合、\n",
    "- 状態$s_t$において行動$a_t$を取った後に、「左の色が黒」「右の色が黒」となった場合に報酬$r_{t+1}=+1$を与え、\n",
    "- 「左の色が白」「右の色が白」となった場合に報酬$r_{t+1}=-1$を与え、\n",
    "- それ以外の場合は、報酬を与えない\n",
    "\n",
    "ように設定しています（即時報酬とも呼ばれます）。\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/reward.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一方で、「Q値 $Q(s, a)$」は、**将来の報酬を見越して**、状態$s$において行動$a$を取った場合の価値を表します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "tips: 報酬とQ値の違いの具体例\n",
    "\n",
    "報酬とQ値を具体例で比較してみます。\n",
    "今回の例だと少し分かりにくいので、「囲碁」の例を用いて説明します。\n",
    "- 囲碁の場合、報酬は「勝敗」です。勝った場合に報酬$r=+1$、負けた場合に報酬$r=-1$を与えます。\n",
    "- 一方で、Q値は「局面$s$において、次の手$a$を打った場合に、最終的に勝つ確率」を表します\n",
    "\n",
    "2015年に、DeepMindが開発した「AlphaGo」は、Q値を用いて囲碁の強さを学習し、世界チャンピオンを破ることに成功しました。\n",
    "これは、「一見その瞬間では不利に見えるようでも、将来的に有利になる手を打つ（=**将来の報酬（勝利）を最大化するような行動を取る**）」という戦略を（人間のプロ棋士を超えるほどの）高い精度で学習した結果です。\n",
    "\n",
    "なお、AlphaGoの場合は、Deep Q-Network（DQN）という、Q学習にニューラルネットワークを組み合わせた手法を用いています。\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Q値の更新](#toc0_)\n",
    "\n",
    "Q学習では、以下の式でQ値を更新することを繰り返すことで学習を行います。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "ここで、各変数の意味は以下の通りです。\n",
    "- $\\alpha$：学習率\n",
    "- $r_{t+1}$：状態$s_t$において、行動$a_t$を取った直後に得られる報酬\n",
    "- $\\gamma$：割引率（未来の報酬をどれだけ重視するか）\n",
    "- $\\max_{a} Q(s_{t+1}, a)$：状態$s_{t+1}$において、取りうる行動$a$の中で最もQ値が高いもの\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "　なお、$r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)$の部分は、「TD誤差」と呼ばれ、  \n",
    "TD誤差を$\\delta = r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)$として以下のように定義することもあります（コード内でもこの定義を使用しています）。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/td_error.png\" width=\"60%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "特に、$r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a)$ の部分を「TDターゲット」と呼び、$Q(s_t, a_t)$がこのTDターゲットに近づくようにQ値を更新していきます。\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/td_target.png\" width=\"60%\">\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "---\n",
    "\n",
    "tips:\n",
    "\n",
    "他の強化学習手法に、SARSA（State-Action-Reward-State-Action）という手法があります。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_learn_and_sarsa.png\" width=\"60%\">\n",
    "\n",
    "Q学習が「最もQ値が高い行動を選択する」のに対して、SARSAは「実際に取った行動に対するQ値を更新する」という違いがあります。\n",
    "\n",
    "---\n",
    "\n",
    "--- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Qテーブル](#toc0_)\n",
    "\n",
    "ここまでで、Q値について説明しましたが、Q値は各状態$s$、行動$a$のペアごとに存在するため、各Q値は下記のような「Qテーブル」と呼ばれるテーブルで管理されます。\n",
    " \n",
    "| states \\ actions | $a_0$ | $a_1$ |...| $a_n$ |\n",
    "|:-----------------:|:-----:|:-----:|:-:|:-----:|\n",
    "| $s_0$             | $Q(s_0, a_0)$ | $Q(s_0, a_1)$ |...| $Q(s_0, a_n)$ |\n",
    "| $s_1$             | $Q(s_1, a_0)$ | $Q(s_1, a_1)$ |...| $Q(s_1, a_n)$ |\n",
    "| ...               | ... | ... |...| ... |\n",
    "| $s_m$             | $Q(s_m, a_0)$ | $Q(s_m, a_1)$ |...| $Q(s_m, a_n)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のライントレース問題におけるQテーブルは下図のようになります。\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_table.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のライントレース問題の場合、\n",
    "- 「黒黒」の場合は直進\n",
    "- 「白黒」の場合は右折\n",
    "- 「黒白」の場合は左折\n",
    "\n",
    "という風に、走行する必要があるので、学習後のQテーブルは下図のようになれば良さそうです（Highと書いてある部分が、学習後にQ値が高くなるべき部分です）。\n",
    "\n",
    "<img src=\"../images/MobileRobot/mobile_robot_reinforcement_q_learning/q_table_after_learning.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "\n",
    "※ 今回は勉強用に、学習後のQテーブルを予想できるような簡単な問題設定にしています。  \n",
    "実際は、複雑な問題になるとQテーブルを予想することは困難なので、適切に報酬などを設定することが重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class Actions(IntEnum):\n",
    "    \"\"\"\n",
    "    行動の定義（Q-tableの列）\n",
    "    \"\"\"\n",
    "    STRAIGHT = 0\n",
    "    TURN_LEFT = 1\n",
    "    TURN_RIGHT = 2\n",
    "\n",
    "class States(IntEnum):\n",
    "    \"\"\"\n",
    "    状態の定義（Q-tableの行）\n",
    "    \"\"\"\n",
    "    BLACK_BLACK = 0\n",
    "    WHITE_BLACK = 1\n",
    "    BLACK_WHITE = 2\n",
    "    WHITE_WHITE = 3\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, q_table, LEARNING_RATE=0.1, DISCOUNT_RATE=0.9, EPSILON=0.1):\n",
    "        \"\"\"\n",
    "        Q学習の初期化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_table : list\n",
    "            Qテーブル\n",
    "        LEARNING_RATE : float\n",
    "            学習率\n",
    "        DISCOUNT_RATE : float\n",
    "            割引率\n",
    "        EPSILON : float\n",
    "            ランダム行動を選択する確率\n",
    "        \"\"\" \n",
    "        self.q_table = q_table\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.DISCOUNT_RATE = DISCOUNT_RATE\n",
    "        self.EPSILON = EPSILON   \n",
    "\n",
    "    def get_state(self, car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix):\n",
    "        \"\"\"\n",
    "        状態の取得\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        car_id : int\n",
    "            ロボットのID\n",
    "        CAMERA_IDX : int\n",
    "            カメラリンクのインデックス\n",
    "        CAMERA_TARGET_IDX : int\n",
    "            注視点用の仮想的なリンクのインデックス\n",
    "        projection_matrix : list\n",
    "            プロジェクション行列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : int\n",
    "            状態 s_t\n",
    "        \"\"\"\n",
    "        # camera_up_vectorのデフォルトの方向\n",
    "        camera_up_vector = np.array([0, -1, 0])\n",
    "        \n",
    "        # カメラリンクの位置を取得\n",
    "        camera_link_pose = pybullet.getLinkState(car_id, CAMERA_IDX)[0]\n",
    "\n",
    "        # 注視点用の仮想的なリンクの位置を取得\n",
    "        camera_target_link_pose = pybullet.getLinkState(car_id, CAMERA_TARGET_IDX)[0] \n",
    "\n",
    "        # cameraUpVectorを移動ロボットの姿勢に合わせて回転\n",
    "        mobile_robot_roll, mobile_robot_pitch, mobile_robot_yaw = pybullet.getEulerFromQuaternion(pybullet.getLinkState(car_id, CAMERA_IDX)[1])\n",
    "        R = Rz(np.deg2rad(90.0) + mobile_robot_yaw)@Ry(mobile_robot_pitch)@Rx(mobile_robot_roll)\n",
    "        rotate_camera_up_vector = R@camera_up_vector\n",
    "\n",
    "        # カメラリンク -> 注視点用の仮想的なリンク 方向のviewMatrixを取得\n",
    "        view_matrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]],cameraTargetPosition=[camera_target_link_pose[0], camera_target_link_pose[1], camera_target_link_pose[2]],cameraUpVector=[rotate_camera_up_vector[0], rotate_camera_up_vector[1], rotate_camera_up_vector[2]])\n",
    "        \n",
    "        # ラインの画像を取得\n",
    "        width, height, rgb_img, _, _ = pybullet.getCameraImage(600, 300, view_matrix, projection_matrix, renderer=pybullet.ER_BULLET_HARDWARE_OPENGL, shadow=0, flags=pybullet.ER_NO_SEGMENTATION_MASK)\n",
    "\n",
    "        # 画像の2値化\n",
    "        img = np.reshape(rgb_img, (height, width, 4))  # 取得した画像を4チャンネルのnumpy配列に変換\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # グレースケールに変換\n",
    "        _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)  # 2値化\n",
    "\n",
    "        # 画像の中心から少し左の位置にあるピクセルの色を取得\n",
    "        color_left = binary_img[height//2, width//2 - 150]\n",
    "\n",
    "        # 画像の中心から少し右の位置にあるピクセルの色を取得\n",
    "        color_right = binary_img[height//2, width//2 + 150]\n",
    "\n",
    "        # 状態の取得\n",
    "        if color_left == 0 and color_right == 0:\n",
    "            state = States.BLACK_BLACK\n",
    "        elif color_left == 255 and color_right == 0:\n",
    "            state = States.WHITE_BLACK\n",
    "        elif color_left == 0 and color_right == 255:\n",
    "            state = States.BLACK_WHITE\n",
    "        elif color_left == 255 and color_right == 255:\n",
    "            state = States.WHITE_WHITE\n",
    "        return state\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        行動の取得\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            状態 s_t\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            行動 a_t\n",
    "        \"\"\"\n",
    "\n",
    "        # ε-greedy法による行動選択\n",
    "        if np.random.uniform(0, 1) < self.EPSILON:\n",
    "            # epsilonの確率でランダムに行動を選択\n",
    "            action = np.random.choice(list(Actions))\n",
    "        else:\n",
    "            # 1-epsilonの確率でQ値が最大となる行動を選択\n",
    "            action = Actions(np.argmax(self.q_table[state][:]))\n",
    "        return action\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        報酬の取得\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            状態 s_t\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            報酬 r_t\n",
    "        \"\"\"\n",
    "        if state == States.BLACK_BLACK:\n",
    "            reward = 1\n",
    "        elif state == States.BLACK_WHITE:\n",
    "            reward = 0\n",
    "        elif state == States.WHITE_BLACK:\n",
    "            reward = 0\n",
    "        elif state == States.WHITE_WHITE:\n",
    "            reward = -1\n",
    "        return reward\n",
    "    \n",
    "    def update_q_table(self, state, action, state_next, reward):\n",
    "        \"\"\"\n",
    "        Qテーブルの更新\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : int\n",
    "            時刻tの状態 s_t\n",
    "        action : int\n",
    "            時刻tの行動 a_t\n",
    "        state_next : int\n",
    "            時刻t+1の状態 s_{t+1}\n",
    "        reward : float\n",
    "            時刻t+1の報酬 r_{t+1}\n",
    "            ※「時刻tの行動a_t」を取った結果、「時刻tの状態s{t}」→ 「時刻t+1の状態s_{t+1}」に遷移した結果得られる報酬なので、添え字はt+1となる\n",
    "        \"\"\"\n",
    "        # 「時刻t+1の状態s_{t+1}」で可能な「行動a」の中で最大のQ値を求める\n",
    "        max_q_value = max(self.q_table[state_next][:])\n",
    "\n",
    "        # TD誤差の計算\n",
    "        td_error = reward + self.DISCOUNT_RATE * max_q_value - self.q_table[state][action]\n",
    "                                                            \n",
    "        # Qテーブルの更新\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.LEARNING_RATE * td_error\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Qテーブルの初期化](#toc0_)\n",
    "\n",
    "Qテーブルを初期化します。\n",
    "\n",
    "学習を途中で中断した場合でも、このQテーブルの値を保存しておくことで、学習を途中から再開することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((len(States), len(Actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Q学習のパラメータ設定](#toc0_)\n",
    "\n",
    "Q学習におけるパラメータを設定します。\n",
    "\n",
    "以下のパラメータを変更することで、学習の速度や性能が変わります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_MAX = 1000 # 最大エピソード数\n",
    "STEP_MAX = 4000 # 1エピソードあたりの最大ステップ数\n",
    "LEARNING_RATE = 0.01 # 学習率\n",
    "DISCOUNT_RATE = 0.9 # 割引率\n",
    "EPSILON = 0.1 # ε-greedy法のε（ランダム行動を選択する確率）\n",
    "LEARNING_COMPLETE_EPISODE_NUM = 4 # 学習が完了したと判断するエピソード数（今回の場合、移動ロボットがラインから外れずに「LEARNING_COMPLETE_EPISODE_NUM回連続で」一周出来た場合に学習完了とする）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Q学習の実行](#toc0_)\n",
    "\n",
    "Q学習は、以下の手順で実行されます。\n",
    "\n",
    "1. 初期状態$s_t$を取得\n",
    "2. Qテーブルから、状態$s_t$において最もQ値が高い行動$a_t$を選択し、実行\n",
    "    - ただし、一定の確率でランダムな行動を選択する（ε-greedy法）\n",
    "3. 次の状態$s_{t+1}$を取得\n",
    "4. 報酬$r_{t+1}$を取得\n",
    "5. Q値$Q(s_t, a_t)$を更新し、Qテーブルに反映\n",
    "6. 状態$s_t$を$s_{t+1}$に更新\n",
    "7. 学習が完了するか、最大繰り返し回数に達するまで、2.〜6.を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m240.\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 次の状態を取得\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m state_next \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcar_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCAMERA_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCAMERA_TARGET_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 報酬を取得\u001b[39;00m\n\u001b[1;32m    106\u001b[0m reward \u001b[38;5;241m=\u001b[39m q_learning\u001b[38;5;241m.\u001b[39mget_reward(state_next)\n",
      "Cell \u001b[0;32mIn[4], line 82\u001b[0m, in \u001b[0;36mQLearning.get_state\u001b[0;34m(self, car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\u001b[0m\n\u001b[1;32m     79\u001b[0m view_matrix \u001b[38;5;241m=\u001b[39m pybullet\u001b[38;5;241m.\u001b[39mcomputeViewMatrix(cameraEyePosition\u001b[38;5;241m=\u001b[39m[camera_link_pose[\u001b[38;5;241m0\u001b[39m], camera_link_pose[\u001b[38;5;241m1\u001b[39m], camera_link_pose[\u001b[38;5;241m2\u001b[39m]],cameraTargetPosition\u001b[38;5;241m=\u001b[39m[camera_target_link_pose[\u001b[38;5;241m0\u001b[39m], camera_target_link_pose[\u001b[38;5;241m1\u001b[39m], camera_target_link_pose[\u001b[38;5;241m2\u001b[39m]],cameraUpVector\u001b[38;5;241m=\u001b[39m[rotate_camera_up_vector[\u001b[38;5;241m0\u001b[39m], rotate_camera_up_vector[\u001b[38;5;241m1\u001b[39m], rotate_camera_up_vector[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# ラインの画像を取得\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m width, height, rgb_img, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetCameraImage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mER_BULLET_HARDWARE_OPENGL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshadow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpybullet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mER_NO_SEGMENTATION_MASK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 画像の2値化\u001b[39;00m\n\u001b[1;32m     85\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(rgb_img, (height, width, \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# 取得した画像を4チャンネルのnumpy配列に変換\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ロボットを初期位置にセット\n",
    "car_start_pos = [3.0, 0, 0.1]\n",
    "car_start_orientation = pybullet.getQuaternionFromEuler([0.0, 0.0, -math.pi/2])\n",
    "pybullet.resetBasePositionAndOrientation(car_id, car_start_pos, car_start_orientation)\n",
    "\n",
    "\n",
    "# ボトムカメラの設定\n",
    "projection_matrix = pybullet.computeProjectionMatrixFOV(fov=140.0,aspect=1.0,nearVal=0.04,farVal=100)\n",
    "    \n",
    "# リンクのインデックス\n",
    "CAMERA_IDX = 8\n",
    "CAMERA_TARGET_IDX = 9\n",
    "\n",
    "# ジョイントのインデックス\n",
    "RIGHT_WHEEL_JOINT_IDX = 0\n",
    "LEFT_WHEEL_JOINT_IDX = 1\n",
    "\n",
    "# 直進時の速度\n",
    "BASE_SPEED = 30\n",
    "\n",
    "# Q学習の初期化\n",
    "q_learning = QLearning(q_table, LEARNING_RATE, DISCOUNT_RATE, EPSILON)\n",
    "episode_complete_cnt = 0 # 移動ロボットがラインを外れずに「連続で」一周した回数\n",
    "\n",
    "# 学習開始\n",
    "for episode in range(EPISODE_MAX):\n",
    "\n",
    "    # 学習が完了したら終了\n",
    "    if episode_complete_cnt == LEARNING_COMPLETE_EPISODE_NUM:\n",
    "        pybullet.removeAllUserDebugItems()\n",
    "        pybullet.addUserDebugText(f\"Learning is complete!!\", [-2.3, 6.0, 0.1], textSize=2, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"Episode: {episode}\", [-0.7, 5.5, 0.1], textSize=1.5, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"Q table\", [-0.5, 5.0, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"| STATE \\ ACTION | STRAIGHT | TURN LEFT | TURN RIGHT |\", [-3.9, 4.6, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"----------------------------------------------------------------------------\", [-3.9, 4.4, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   BLACK-BLACK   |    {q_table[0][0]:.2f}       |     {q_table[0][1]:.2f}        |      {q_table[0][2]:.2f}        |\", [-3.9, 4.2, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   WHITE-BLACK   |    {q_table[1][0]:.2f}       |     {q_table[1][1]:.2f}        |      {q_table[1][2]:.2f}        |\", [-3.9, 3.8, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   BLACK-WHITE   |    {q_table[2][0]:.2f}       |     {q_table[2][1]:.2f}        |      {q_table[2][2]:.2f}        |\", [-3.9, 3.4, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        pybullet.addUserDebugText(f\"|   WHITE-WHITE   |    {q_table[3][0]:.2f}       |     {q_table[3][1]:.2f}        |      {q_table[3][2]:.2f}        |\", [-3.9, 3.0, 0.1], textSize=1.3, textColorRGB=[1,0,0])\n",
    "        break\n",
    "\n",
    "    # 移動ロボットを初期位置、速度0にセット\n",
    "    # エピソードごとに走行の向きを変える\n",
    "    if episode % 2 == 0:\n",
    "        car_start_pos = [3.0, 0, 0.1]\n",
    "        car_yaw = -math.pi/2\n",
    "    else:\n",
    "        car_yaw = math.pi/2\n",
    "        car_start_pos = [3.0, 0, 0.1]\n",
    "    car_start_orientation = pybullet.getQuaternionFromEuler([0.0, 0.0, car_yaw])\n",
    "    pybullet.resetBasePositionAndOrientation(car_id, car_start_pos, car_start_orientation)\n",
    "    pybullet.setJointMotorControl2(car_id, RIGHT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=0)\n",
    "    pybullet.setJointMotorControl2(car_id, LEFT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=0)\n",
    "\n",
    "    # エピソードごとにQテーブルを表示\n",
    "    # (実際は、ステップごとに毎回Qテーブルは更新されているが、addUserDebugTextは処理が重いため、エピソードごとに表示)\n",
    "    pybullet.removeAllUserDebugItems()\n",
    "    pybullet.addUserDebugText(f\"Episode: {episode+1}\", [-0.7, 5.5, 0.1], textSize=1.5, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"Q table\", [-0.5, 5.0, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"| STATE \\ ACTION | STRAIGHT | TURN LEFT | TURN RIGHT |\", [-3.9, 4.6, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"----------------------------------------------------------------------------\", [-3.9, 4.4, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   BLACK-BLACK   |    {q_table[0][0]:.2f}       |     {q_table[0][1]:.2f}        |      {q_table[0][2]:.2f}        |\", [-3.9, 4.2, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   WHITE-BLACK   |    {q_table[1][0]:.2f}       |     {q_table[1][1]:.2f}        |      {q_table[1][2]:.2f}        |\", [-3.9, 3.8, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   BLACK-WHITE   |    {q_table[2][0]:.2f}       |     {q_table[2][1]:.2f}        |      {q_table[2][2]:.2f}        |\", [-3.9, 3.4, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "    pybullet.addUserDebugText(f\"|   WHITE-WHITE   |    {q_table[3][0]:.2f}       |     {q_table[3][1]:.2f}        |      {q_table[3][2]:.2f}        |\", [-3.9, 3.0, 0.1], textSize=1.3, textColorRGB=[1,0,1])\n",
    "\n",
    "    # 1. 初期状態を取得\n",
    "    state = q_learning.get_state(car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\n",
    "    \n",
    "    # 1エピソードで最大STEP_MAX回の行動を取る\n",
    "    for step in range(STEP_MAX):\n",
    "\n",
    "        # 2. Qテーブルから行動を選択し、実行\n",
    "        action = q_learning.get_action(state)\n",
    "        if action == Actions.STRAIGHT:\n",
    "            left_speed = BASE_SPEED\n",
    "            right_speed = BASE_SPEED\n",
    "        elif action == Actions.TURN_LEFT:\n",
    "            left_speed = BASE_SPEED - 10\n",
    "            right_speed = BASE_SPEED\n",
    "        elif action == Actions.TURN_RIGHT:\n",
    "            left_speed = BASE_SPEED\n",
    "            right_speed = BASE_SPEED - 10\n",
    "        pybullet.setJointMotorControl2(car_id, RIGHT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=right_speed)\n",
    "        pybullet.setJointMotorControl2(car_id, LEFT_WHEEL_JOINT_IDX, pybullet.VELOCITY_CONTROL, targetVelocity=left_speed)\n",
    "\n",
    "        # 一時刻分、シミュレーションを進める\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(1./240.)\n",
    "\n",
    "        # 3. 次の状態を取得\n",
    "        state_next = q_learning.get_state(car_id, CAMERA_IDX, CAMERA_TARGET_IDX, projection_matrix)\n",
    "\n",
    "        # 4. 報酬を取得\n",
    "        reward = q_learning.get_reward(state_next)\n",
    "\n",
    "        # 5. Qテーブルの更新\n",
    "        q_learning.update_q_table(state, action, state_next, reward)\n",
    "\n",
    "        # 6. 状態を更新\n",
    "        state = state_next\n",
    "\n",
    "        # 移動ロボットがラインを外れたらエピソードを終了\n",
    "        if state_next == States.WHITE_WHITE:\n",
    "            episode_complete_cnt = 0\n",
    "            break\n",
    "\n",
    "        # 移動ロボットが、ラインを一周したら（＝スタート地点付近に戻ってきたら）エピソードを終了\n",
    "        current_car_pos = pybullet.getBasePositionAndOrientation(car_id)[0]\n",
    "        current_to_start_distance = (car_start_pos[0] - current_car_pos[0])**2 + (car_start_pos[1] - current_car_pos[1])**2\n",
    "        if current_to_start_distance < 0.1**2 and step > 1000:\n",
    "            episode_complete_cnt += 1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[おまけ：ライントレース用の画像生成](#toc0_)\n",
    "\n",
    "以下コードを実行すると、指定の半径の円を描画した画像を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X connection to :0 broken (explicit kill or server shutdown).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "dpi = 100\n",
    "\n",
    "plane_width = 10 \n",
    "plane_height = 10\n",
    "line_radius = 3\n",
    "\n",
    "image_width = (plane_width * dpi)\n",
    "image_height = (plane_height * dpi)\n",
    "\n",
    "image = Image.new('RGB', (image_width, image_height), (255, 255, 255))\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "def gazebo_to_image_coords(x, y):\n",
    "    x_image = int(((x + plane_width/2) / plane_width) * image_width)\n",
    "    y_image = int((1 - (y + plane_height/2) / plane_height) * image_height)\n",
    "    return x_image, y_image\n",
    "\n",
    "center_x_gazebo, center_y_gazebo = 0, 0\n",
    "\n",
    "center_x_image, center_y_image = gazebo_to_image_coords(center_x_gazebo, center_y_gazebo)\n",
    "radius_image = int(line_radius * image_width / plane_width)\n",
    "\n",
    "draw.ellipse((center_x_image - radius_image, center_y_image - radius_image, center_x_image + radius_image, center_y_image + radius_image), outline=\"black\", width=15)\n",
    "\n",
    "image.save('../texture/line_trace_ground.png', dpi=(dpi, dpi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
